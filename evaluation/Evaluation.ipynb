{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCGERDM6UKd_"
   },
   "source": [
    "# **Topic Model Evaluation**\n",
    "Here, you will find the code needed to run the experiments of the paper:\n",
    "\n",
    "*BERTopic: Neural topic modeling with a class-based TF-IDF procedure*.\n",
    "\n",
    "The package itself can be found [here](https://github.com/MaartenGr/BERTopic) and the repository for evaluation [here]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFwg78cJ0BFg"
   },
   "source": [
    "## **Installation**\n",
    "First, we need to install a few packages in order to run our experiments. Most of the packages are installed through the `tm_evaluation` package of which [OCTIS](https://github.com/MIND-Lab/OCTIS) is an important component. \n",
    "\n",
    "You can install the evaluation package with `pip install .` from the root. To additionally install CTM run `pip install .[ctm]`To install BERTopic, run `pip install bertopic==v0.9.4` after installing the base package or use `pip install .[bertopic]`. Top2Vec should be installed with `pip install top2vec==v1.0.26` after installing the base package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLesRC-17zN9"
   },
   "source": [
    "To run a faster version of LDAseq for dynamic topic modeling, we need to uninstall gensim and install a specific merge that allows for this speed-up. First, run `pip uninstall gensim -y`, then, run `pip install git+https://github.com/RaRe-Technologies/gensim.git@refs/pull/3172/merge`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fisNVVa977e-"
   },
   "source": [
    "**NOTE**: After installing the above packages, make sure to restart the runtime otherwise you are likely to run into issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQnAdKJ-Do1S"
   },
   "source": [
    "#  1. **Data**\n",
    "Some of the data can be accessed through OCTIS, such as the `20NewsGroup` and `BBC_News` datasets. Other datasets, however, are downloaded and then run through OCTIS in order to be used in their pipeline. \n",
    "\n",
    "The datasets that we are going to be preparing are: \n",
    "* Trump's tweets\n",
    "* United Nations general debates between 2006 and 2015 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting octis\n",
      "  Downloading octis-1.13.1-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Collecting gensim==4.2.0 (from octis)\n",
      "  Downloading gensim-4.2.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: nltk in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from octis) (3.6.5)\n",
      "Requirement already satisfied: pandas in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from octis) (2.1.3)\n",
      "Requirement already satisfied: spacy in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from octis) (3.5.1)\n",
      "Collecting scikit-learn==1.1.0 (from octis)\n",
      "  Downloading scikit_learn-1.1.0-cp39-cp39-macosx_10_13_x86_64.whl.metadata (10 kB)\n",
      "Collecting scikit-optimize>=0.8.1 (from octis)\n",
      "  Downloading scikit_optimize-0.10.1-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: matplotlib in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from octis) (3.8.2)\n",
      "Requirement already satisfied: torch in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from octis) (2.1.1)\n",
      "Collecting numpy==1.23.0 (from octis)\n",
      "  Downloading numpy-1.23.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting libsvm (from octis)\n",
      "  Downloading libsvm-3.23.0.4.tar.gz (170 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.6/170.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: flask in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from octis) (3.0.0)\n",
      "Requirement already satisfied: sentence-transformers in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from octis) (2.2.2)\n",
      "Requirement already satisfied: requests in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from octis) (2.31.0)\n",
      "Collecting tomotopy (from octis)\n",
      "  Downloading tomotopy-0.12.7.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from gensim==4.2.0->octis) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from gensim==4.2.0->octis) (6.3.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.1.0->octis) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.1.0->octis) (2.2.0)\n",
      "Collecting pyaml>=16.9 (from scikit-optimize>=0.8.1->octis)\n",
      "  Downloading pyaml-24.4.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from scikit-optimize>=0.8.1->octis) (23.2)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from flask->octis) (3.0.1)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from flask->octis) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from flask->octis) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from flask->octis) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from flask->octis) (1.6.3)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from flask->octis) (6.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->octis) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->octis) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->octis) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->octis) (1.3.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->octis) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->octis) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->octis) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->octis) (6.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from nltk->octis) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from nltk->octis) (4.66.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from pandas->octis) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from pandas->octis) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from requests->octis) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from requests->octis) (3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from requests->octis) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from requests->octis) (2021.10.8)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers->octis) (4.33.2)\n",
      "Requirement already satisfied: torchvision in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers->octis) (0.16.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers->octis) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers->octis) (0.19.4)\n",
      "Requirement already satisfied: filelock in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from torch->octis) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from torch->octis) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from torch->octis) (1.9)\n",
      "Requirement already satisfied: networkx in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from torch->octis) (2.6.3)\n",
      "Requirement already satisfied: fsspec in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from torch->octis) (2023.10.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from spacy->octis) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from spacy->octis) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from spacy->octis) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from spacy->octis) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from spacy->octis) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from spacy->octis) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from spacy->octis) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from spacy->octis) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from spacy->octis) (2.0.8)\n",
      "Collecting typer<0.8.0,>=0.3.0 (from spacy->octis)\n",
      "  Downloading typer-0.7.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from spacy->octis) (0.10.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy->octis)\n",
      "  Downloading pydantic-1.10.15-cp39-cp39-macosx_10_9_x86_64.whl.metadata (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from spacy->octis) (58.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from spacy->octis) (3.3.0)\n",
      "Requirement already satisfied: six in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib->octis) (1.16.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->octis) (6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->flask->octis) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from Jinja2>=3.1.2->flask->octis) (2.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy->octis) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy->octis) (0.0.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->octis) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->octis) (0.4.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch->octis) (1.2.1)\n",
      "Downloading octis-1.13.1-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gensim-4.2.0-cp39-cp39-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.0-cp39-cp39-macosx_10_9_x86_64.whl (18.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading scikit_learn-1.1.0-cp39-cp39-macosx_10_13_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_optimize-0.10.1-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyaml-24.4.0-py3-none-any.whl (24 kB)\n",
      "Downloading pydantic-1.10.15-cp39-cp39-macosx_10_9_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Building wheels for collected packages: libsvm, tomotopy\n",
      "  Building wheel for libsvm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for libsvm: filename=libsvm-3.23.0.4-cp39-cp39-macosx_10_9_x86_64.whl size=183242 sha256=c5b2b0e530f4118474214ef1c3537252f3f9d850b561f9c7bbcbdd9ac72fc9dd\n",
      "  Stored in directory: /Users/patrickjonathan/Library/Caches/pip/wheels/c1/ce/25/0d50035499973fcbcc407fcb897d53e47b6eb4601308789aa6\n",
      "  Building wheel for tomotopy (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[51 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-3.9\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/_version.py -> build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/coherence.py -> build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/__init__.py -> build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/_call_utils.py -> build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/_summary.py -> build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/utils.py -> build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/label.py -> build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/_show_progress.py -> build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing tomotopy.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to tomotopy.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to tomotopy.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to tomotopy.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'tomotopy.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '*.c' under directory 'src'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching 'LICENSE.txt'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'tomotopy.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/auto_labeling_code.rst -> build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/auto_labeling_code_with_porter.rst -> build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/documentation.kr.rst -> build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/documentation.rst -> build/lib.macosx-10.9-x86_64-3.9/tomotopy\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-3.9/tomotopy/viewer\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/viewer/__init__.py -> build/lib.macosx-10.9-x86_64-3.9/tomotopy/viewer\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/viewer/template.html -> build/lib.macosx-10.9-x86_64-3.9/tomotopy/viewer\n",
      "  \u001b[31m   \u001b[0m copying tomotopy/viewer/viewer_server.py -> build/lib.macosx-10.9-x86_64-3.9/tomotopy/viewer\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building '_tomotopy' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-10.9-x86_64-3.9\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-10.9-x86_64-3.9/src\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-10.9-x86_64-3.9/src/Labeling\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-10.9-x86_64-3.9/src/TopicModel\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-10.9-x86_64-3.9/src/python\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/patrickjonathan/opt/anaconda3/include -I/Users/patrickjonathan/opt/anaconda3/include -fPIC -O2 -isystem /Users/patrickjonathan/opt/anaconda3/include -DMODULE_NAME=PyInit__tomotopy -Iinclude -I/Users/patrickjonathan/opt/anaconda3/lib/python3.9/site-packages/numpy/core/include -I/Users/patrickjonathan/opt/anaconda3/include/python3.9 -c src/Labeling/FoRelevance.cpp -o build/temp.macosx-10.9-x86_64-3.9/src/Labeling/FoRelevance.o -std=c++1y -O3 -fpermissive -stdlib=libc++ -Wno-unused-variable -Wno-switch -arch x86_64 -msse2\n",
      "  \u001b[31m   \u001b[0m In file included from src/Labeling/FoRelevance.cpp:4:\n",
      "  \u001b[31m   \u001b[0m In file included from src/Labeling/FoRelevance.h:4:\n",
      "  \u001b[31m   \u001b[0m In file included from src/Labeling/Labeler.h:4:\n",
      "  \u001b[31m   \u001b[0m In file included from src/Labeling/../TopicModel/TopicModel.hpp:5:\n",
      "  \u001b[31m   \u001b[0m In file included from src/Labeling/../TopicModel/../Utils/Dictionary.h:9:\n",
      "  \u001b[31m   \u001b[0m src/Labeling/../TopicModel/../Utils/serializer.hpp:10:10: fatal error: 'Eigen/Dense' file not found\n",
      "  \u001b[31m   \u001b[0m #include <Eigen/Dense>\n",
      "  \u001b[31m   \u001b[0m          ^~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m 1 error generated.\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/clang' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for tomotopy\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for tomotopy\n",
      "Successfully built libsvm\n",
      "Failed to build tomotopy\n",
      "\u001b[31mERROR: Could not build wheels for tomotopy, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'octis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lq/jbrfmdz51qbcss6qvy3ptnrw0000gn/T/ipykernel_40471/2192899066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install octis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Thesis_Data/BERTopic_evaluation/evaluation/evaluation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moctis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mETM\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mETM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moctis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLDA\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moctis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNMF\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNMF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'octis'"
     ]
    }
   ],
   "source": [
    "!pip install octis\n",
    "from evaluation import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLvF2devjGZt"
   },
   "source": [
    "## Trump\n",
    "The data can be found here: https://www.thetrumparchive.com/faq\n",
    "\n",
    "Using our `DataLoader` we can prepare the documents and save them in an OCTIS-based format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-X1poWssOoQ",
    "outputId": "5c6b14ff-0ce3-43f0-d18e-fa3952cba31b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocab\n",
      "53637\n",
      "words filtering done\n",
      "CPU times: user 2min 44s, sys: 1.81 s, total: 2min 46s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloader = DataLoader(dataset=\"trump\").prepare_docs(save=\"trump.txt\").preprocess_octis(output_folder=\"trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV-GDOko9CaC"
   },
   "source": [
    "Additionally, there isa DTM variant that creates 10 timesteps to be used in the dynamic topic modeling experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9P7qS9Qp56K"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dataloader = DataLoader(dataset=\"trump_dtm\").prepare_docs(save=\"trump_dtm.txt\").preprocess_octis(output_folder=\"trump_dtm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DFzHi8Tp_6Z"
   },
   "source": [
    "## United Nations\n",
    "\n",
    "The transcriptions of the United Nations (UN) general debates between 2006 and 2015. The data can be found here: https://runestone.academy/runestone/books/published/httlads/_static/un-general-debates.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-exAptIjqBrO",
    "outputId": "56773d61-3810-4ef4-9f20-9ee339c08faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocab\n",
      "69447\n",
      "words filtering done\n",
      "CPU times: user 22min, sys: 21.5 s, total: 22min 21s\n",
      "Wall time: 22min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloader = DataLoader(dataset=\"un_dtm\").prepare_docs(save=\"un_dtm.txt\").preprocess_octis(output_folder=\"un_dtm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRJJvgHyjYa6"
   },
   "source": [
    "# 2. **Evaluation**\n",
    "After preparing our data, we can start evaluating the topic models as used in the experiments. OCTIS already has a number of models prepared that we can use directly as shown below. \n",
    "\n",
    "First, we specify what the dataset is and whether that was a custom dataset not found in OCTIS. To run our custom trump dataset, we run `dataset, custom = \"trump\", True`. In contrast, if we are to use the prepackaged 20NewsGroup dataset, we run `dataset, custom = \"20NewsGroup\", False` instead. \n",
    "\n",
    "The OCTIS datasets can be found [here](https://github.com/MIND-Lab/OCTIS#available-datasets). \n",
    "\n",
    "Second, we define a number of parameters to be used for the model. It uses the following format: \n",
    "\n",
    "`params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}`\n",
    "\n",
    "were we define a number of topics to loop over and calculate the evluation metrics but also define a number of parameters used in the models. \n",
    "\n",
    "#### **Parameters**\n",
    "The parameters for LDA and NMF:\n",
    "\n",
    "\n",
    "```python\n",
    "params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}`\n",
    "```\n",
    "\n",
    "The parameters for Top2Vec:\n",
    "\n",
    "```python\n",
    "params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "          \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                            'metric': 'euclidean',\n",
    "                            'cluster_selection_method': 'eom'}}\n",
    "```\n",
    "Note that the `min_cluster_size` is 15 for all datasets except BBC_News.\n",
    "\n",
    "The parameters for CTM:\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    \"n_components\": [(i+1)*10 for i in range(5)],\n",
    "    \"contextual_size\":768\n",
    "}\n",
    "```\n",
    "\n",
    "The parameters for BERTopic:\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "    \"min_topic_size\": 15,\n",
    "    \"verbose\": True\n",
    "}\n",
    "```\n",
    "\n",
    "Note that the `min_topic_size` is 15 for all datasets except BBC_News. Note that we do not set a `embedding_model` here. We do this on purpose as we can generate the embeddings beforehand and pass those to BERTopic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOhlHj4SGush"
   },
   "source": [
    "## **OCTIS**\n",
    "Here, we can run the experiments for NMF and LDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hu1kVxBhA4iY"
   },
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOZl2Xoed_3a"
   },
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = \"trump\", True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"NMF\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"NMF_trump_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8s9zjC5A6JD"
   },
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAZ_ckYBeFDY"
   },
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = \"trump\", True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"LDA\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"LDA_trump_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsRdU70OVsA4"
   },
   "source": [
    "## **CTM**\n",
    "Here, we use de CombinedTM of the Contextualized Topic Models:  https://github.com/MilaNLProc/contextualized-topic-models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPUkcq9hXsV8"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    dataset, custom = \"trump\", True\n",
    "    params = {\n",
    "        \"n_components\": [(i+1)*10 for i in range(5)],\n",
    "        \"contextual_size\":768\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"CTM_CUSTOM\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"CTM_trump_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGhfpkBZGtRl"
   },
   "source": [
    "## **BERTopic**\n",
    "\n",
    "To speed up BERTopic, we can generate the embeddings before passing it to the `Trainer`. This way, the same embeddings do not have to be generated 5 times which speeds up evaluation quite a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhFXGK8oxVpi"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Prepare data\n",
    "dataset, custom = \"trump\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb1afGtJEuw0"
   },
   "source": [
    "As show above, we load in the `data` which the data loader and combine the tokens in each document to generate our training data. Then, we pass it to the sentence transformer model of our choice and generate the embeddings. \n",
    "\n",
    "Next, we pass these embeddings to the `bt_embeddings` parameter to speed up training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "II0a3WJP37V0"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    params = {\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 15,\n",
    "        \"diversity\": None,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"BERTopic_trump_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpyBVa1Ka9Hw"
   },
   "source": [
    "## **Top2Vec**\n",
    "Aside from its Doc2Vec backend, we also want to explore its performance using the `\"all-mpnet-base-v2\"` SBERT model as that was used in BERTopic. To do so, we make a very slight change to the core code of Top2Vec, namely replacing all instances of `\"\"distiluse-base-multilingual-cased\"` with `\"all-mpnet-base-v2\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pc-PXbV205R3"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "import umap\n",
    "import hdbscan\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from sklearn.cluster import dbscan\n",
    "import tempfile\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.special import softmax\n",
    "from top2vec import Top2Vec\n",
    "\n",
    "try:\n",
    "    import hnswlib\n",
    "\n",
    "    _HAVE_HNSWLIB = True\n",
    "except ImportError:\n",
    "    _HAVE_HNSWLIB = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_hub as hub\n",
    "    import tensorflow_text\n",
    "\n",
    "    _HAVE_TENSORFLOW = True\n",
    "except ImportError:\n",
    "    _HAVE_TENSORFLOW = False\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    _HAVE_TORCH = True\n",
    "except ImportError:\n",
    "    _HAVE_TORCH = False\n",
    "\n",
    "logger = logging.getLogger('top2vec')\n",
    "logger.setLevel(logging.WARNING)\n",
    "sh = logging.StreamHandler()\n",
    "sh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(sh)\n",
    "\n",
    "\n",
    "def default_tokenizer(doc):\n",
    "    \"\"\"Tokenize documents for training and remove too long/short words\"\"\"\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True)\n",
    "\n",
    "\n",
    "class Top2VecNew(Top2Vec):\n",
    "    \"\"\"\n",
    "    Top2Vec\n",
    "    Creates jointly embedded topic, document and word vectors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_model: string\n",
    "        This will determine which model is used to generate the document and\n",
    "        word embeddings. The valid string options are:\n",
    "            * doc2vec\n",
    "            * universal-sentence-encoder\n",
    "            * universal-sentence-encoder-multilingual\n",
    "            * distiluse-base-multilingual-cased\n",
    "        For large data sets and data sets with very unique vocabulary doc2vec\n",
    "        could produce better results. This will train a doc2vec model from\n",
    "        scratch. This method is language agnostic. However multiple languages\n",
    "        will not be aligned.\n",
    "        Using the universal sentence encoder options will be much faster since\n",
    "        those are pre-trained and efficient models. The universal sentence\n",
    "        encoder options are suggested for smaller data sets. They are also\n",
    "        good options for large data sets that are in English or in languages\n",
    "        covered by the multilingual model. It is also suggested for data sets\n",
    "        that are multilingual.\n",
    "        For more information on universal-sentence-encoder visit:\n",
    "        https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "        For more information on universal-sentence-encoder-multilingual visit:\n",
    "        https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\n",
    "        The distiluse-base-multilingual-cased pre-trained sentence transformer\n",
    "        is suggested for multilingual datasets and languages that are not\n",
    "        covered by the multilingual universal sentence encoder. The\n",
    "        transformer is significantly slower than the universal sentence\n",
    "        encoder options.\n",
    "        For more informati ond istiluse-base-multilingual-cased visit:\n",
    "        https://www.sbert.net/docs/pretrained_models.html\n",
    "    embedding_model_path: string (Optional)\n",
    "        Pre-trained embedding models will be downloaded automatically by\n",
    "        default. However they can also be uploaded from a file that is in the\n",
    "        location of embedding_model_path.\n",
    "        Warning: the model at embedding_model_path must match the\n",
    "        embedding_model parameter type.\n",
    "    documents: List of str\n",
    "        Input corpus, should be a list of strings.\n",
    "    min_count: int (Optional, default 50)\n",
    "        Ignores all words with total frequency lower than this. For smaller\n",
    "        corpora a smaller min_count will be necessary.\n",
    "    speed: string (Optional, default 'learn')\n",
    "        This parameter is only used when using doc2vec as embedding_model.\n",
    "        It will determine how fast the model takes to train. The\n",
    "        fast-learn option is the fastest and will generate the lowest quality\n",
    "        vectors. The learn option will learn better quality vectors but take\n",
    "        a longer time to train. The deep-learn option will learn the best\n",
    "        quality vectors but will take significant time to train. The valid\n",
    "        string speed options are:\n",
    "        \n",
    "            * fast-learn\n",
    "            * learn\n",
    "            * deep-learn\n",
    "    use_corpus_file: bool (Optional, default False)\n",
    "        This parameter is only used when using doc2vec as embedding_model.\n",
    "        Setting use_corpus_file to True can sometimes provide speedup for\n",
    "        large datasets when multiple worker threads are available. Documents\n",
    "        are still passed to the model as a list of str, the model will create\n",
    "        a temporary corpus file for training.\n",
    "    document_ids: List of str, int (Optional)\n",
    "        A unique value per document that will be used for referring to\n",
    "        documents in search results. If ids are not given to the model, the\n",
    "        index of each document in the original corpus will become the id.\n",
    "    keep_documents: bool (Optional, default True)\n",
    "        If set to False documents will only be used for training and not saved\n",
    "        as part of the model. This will reduce model size. When using search\n",
    "        functions only document ids will be returned, not the actual\n",
    "        documents.\n",
    "    workers: int (Optional)\n",
    "        The amount of worker threads to be used in training the model. Larger\n",
    "        amount will lead to faster training.\n",
    "    \n",
    "    tokenizer: callable (Optional, default None)\n",
    "        Override the default tokenization method. If None then\n",
    "        gensim.utils.simple_preprocess will be used.\n",
    "    use_embedding_model_tokenizer: bool (Optional, default False)\n",
    "        If using an embedding model other than doc2vec, use the model's\n",
    "        tokenizer for document embedding. If set to True the tokenizer, either\n",
    "        default or passed callable will be used to tokenize the text to\n",
    "        extract the vocabulary for word embedding.\n",
    "    umap_args: dict (Optional, default None)\n",
    "        Pass custom arguments to UMAP.\n",
    "    hdbscan_args: dict (Optional, default None)\n",
    "        Pass custom arguments to HDBSCAN.\n",
    "    \n",
    "    verbose: bool (Optional, default True)\n",
    "        Whether to print status data during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 documents,\n",
    "                 min_count=50,\n",
    "                 embedding_model='doc2vec',\n",
    "                 embedding_model_path=None,\n",
    "                 speed='learn',\n",
    "                 use_corpus_file=False,\n",
    "                 document_ids=None,\n",
    "                 keep_documents=True,\n",
    "                 workers=None,\n",
    "                 tokenizer=None,\n",
    "                 use_embedding_model_tokenizer=False,\n",
    "                 umap_args=None,\n",
    "                 hdbscan_args=None,\n",
    "                 verbose=True\n",
    "                 ):\n",
    "\n",
    "        if verbose:\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "            self.verbose = True\n",
    "        else:\n",
    "            logger.setLevel(logging.WARNING)\n",
    "            self.verbose = False\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = default_tokenizer\n",
    "\n",
    "        # validate documents\n",
    "        if not (isinstance(documents, list) or isinstance(documents, np.ndarray)):\n",
    "            raise ValueError(\"Documents need to be a list of strings\")\n",
    "        if not all((isinstance(doc, str) or isinstance(doc, np.str_)) for doc in documents):\n",
    "            raise ValueError(\"Documents need to be a list of strings\")\n",
    "        if keep_documents:\n",
    "            self.documents = np.array(documents, dtype=\"object\")\n",
    "        else:\n",
    "            self.documents = None\n",
    "\n",
    "        # validate document ids\n",
    "        if document_ids is not None:\n",
    "            if not (isinstance(document_ids, list) or isinstance(document_ids, np.ndarray)):\n",
    "                raise ValueError(\"Documents ids need to be a list of str or int\")\n",
    "\n",
    "            if len(documents) != len(document_ids):\n",
    "                raise ValueError(\"Document ids need to match number of documents\")\n",
    "            elif len(document_ids) != len(set(document_ids)):\n",
    "                raise ValueError(\"Document ids need to be unique\")\n",
    "\n",
    "            if all((isinstance(doc_id, str) or isinstance(doc_id, np.str_)) for doc_id in document_ids):\n",
    "                self.doc_id_type = np.str_\n",
    "            elif all((isinstance(doc_id, int) or isinstance(doc_id, np.int_)) for doc_id in document_ids):\n",
    "                self.doc_id_type = np.int_\n",
    "            else:\n",
    "                raise ValueError(\"Document ids need to be str or int\")\n",
    "\n",
    "            self.document_ids_provided = True\n",
    "            self.document_ids = np.array(document_ids)\n",
    "            self.doc_id2index = dict(zip(document_ids, list(range(0, len(document_ids)))))\n",
    "        else:\n",
    "            self.document_ids_provided = False\n",
    "            self.document_ids = np.array(range(0, len(documents)))\n",
    "            self.doc_id2index = dict(zip(self.document_ids, list(range(0, len(self.document_ids)))))\n",
    "            self.doc_id_type = np.int_\n",
    "\n",
    "        acceptable_embedding_models = [\"universal-sentence-encoder-multilingual\",\n",
    "                                       \"universal-sentence-encoder\",\n",
    "                                       \"all-mpnet-base-v2\"]\n",
    "\n",
    "        self.embedding_model_path = embedding_model_path\n",
    "\n",
    "        if embedding_model == 'doc2vec':\n",
    "\n",
    "            # validate training inputs\n",
    "            if speed == \"fast-learn\":\n",
    "                hs = 0\n",
    "                negative = 5\n",
    "                epochs = 40\n",
    "            elif speed == \"learn\":\n",
    "                hs = 1\n",
    "                negative = 0\n",
    "                epochs = 40\n",
    "            elif speed == \"deep-learn\":\n",
    "                hs = 1\n",
    "                negative = 0\n",
    "                epochs = 400\n",
    "            elif speed == \"test-learn\":\n",
    "                hs = 0\n",
    "                negative = 5\n",
    "                epochs = 1\n",
    "            else:\n",
    "                raise ValueError(\"speed parameter needs to be one of: fast-learn, learn or deep-learn\")\n",
    "\n",
    "            if workers is None:\n",
    "                pass\n",
    "            elif isinstance(workers, int):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"workers needs to be an int\")\n",
    "\n",
    "            doc2vec_args = {\"vector_size\": 300,\n",
    "                            \"min_count\": min_count,\n",
    "                            \"window\": 15,\n",
    "                            \"sample\": 1e-5,\n",
    "                            \"negative\": negative,\n",
    "                            \"hs\": hs,\n",
    "                            \"epochs\": epochs,\n",
    "                            \"dm\": 0,\n",
    "                            \"dbow_words\": 1}\n",
    "\n",
    "            if workers is not None:\n",
    "                doc2vec_args[\"workers\"] = workers\n",
    "\n",
    "            logger.info('Pre-processing documents for training')\n",
    "\n",
    "            if use_corpus_file:\n",
    "                processed = [' '.join(tokenizer(doc)) for doc in documents]\n",
    "                lines = \"\\n\".join(processed)\n",
    "                temp = tempfile.NamedTemporaryFile(mode='w+t')\n",
    "                temp.write(lines)\n",
    "                doc2vec_args[\"corpus_file\"] = temp.name\n",
    "\n",
    "\n",
    "            else:\n",
    "                train_corpus = [TaggedDocument(tokenizer(doc), [i]) for i, doc in enumerate(documents)]\n",
    "                doc2vec_args[\"documents\"] = train_corpus\n",
    "\n",
    "            logger.info('Creating joint document/word embedding')\n",
    "            self.embedding_model = 'doc2vec'\n",
    "            self.model = Doc2Vec(**doc2vec_args)\n",
    "\n",
    "            if use_corpus_file:\n",
    "                temp.close()\n",
    "\n",
    "        elif embedding_model in acceptable_embedding_models:\n",
    "\n",
    "            self.embed = None\n",
    "            self.embedding_model = embedding_model\n",
    "\n",
    "            self._check_import_status()\n",
    "\n",
    "            logger.info('Pre-processing documents for training')\n",
    "\n",
    "            # preprocess documents\n",
    "            tokenized_corpus = [tokenizer(doc) for doc in documents]\n",
    "\n",
    "            def return_doc(doc):\n",
    "                return doc\n",
    "\n",
    "            # preprocess vocabulary\n",
    "            vectorizer = CountVectorizer(tokenizer=return_doc, preprocessor=return_doc)\n",
    "            doc_word_counts = vectorizer.fit_transform(tokenized_corpus)\n",
    "            words = vectorizer.get_feature_names()\n",
    "            word_counts = np.array(np.sum(doc_word_counts, axis=0).tolist()[0])\n",
    "            vocab_inds = np.where(word_counts > min_count)[0]\n",
    "\n",
    "            if len(vocab_inds) == 0:\n",
    "                raise ValueError(f\"A min_count of {min_count} results in \"\n",
    "                                 f\"all words being ignored, choose a lower value.\")\n",
    "            self.vocab = [words[ind] for ind in vocab_inds]\n",
    "\n",
    "            self._check_model_status()\n",
    "\n",
    "            logger.info('Creating joint document/word embedding')\n",
    "\n",
    "            # embed words\n",
    "            self.word_indexes = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "            self.word_vectors = self._l2_normalize(np.array(self.embed(self.vocab)))\n",
    "\n",
    "            # embed documents\n",
    "            if use_embedding_model_tokenizer:\n",
    "                self.document_vectors = self._embed_documents(documents)\n",
    "            else:\n",
    "                train_corpus = [' '.join(tokens) for tokens in tokenized_corpus]\n",
    "                self.document_vectors = self._embed_documents(train_corpus)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"{embedding_model} is an invalid embedding model.\")\n",
    "\n",
    "        # create 5D embeddings of documents\n",
    "        logger.info('Creating lower dimension embedding of documents')\n",
    "\n",
    "        if umap_args is None:\n",
    "            umap_args = {'n_neighbors': 15,\n",
    "                         'n_components': 5,\n",
    "                         'metric': 'cosine'}\n",
    "\n",
    "        umap_model = umap.UMAP(**umap_args).fit(self._get_document_vectors(norm=False))\n",
    "\n",
    "        # find dense areas of document vectors\n",
    "        logger.info('Finding dense areas of documents')\n",
    "\n",
    "        if hdbscan_args is None:\n",
    "            hdbscan_args = {'min_cluster_size': 15,\n",
    "                            'metric': 'euclidean',\n",
    "                            'cluster_selection_method': 'eom'}\n",
    "\n",
    "        cluster = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)\n",
    "\n",
    "        # calculate topic vectors from dense areas of documents\n",
    "        logger.info('Finding topics')\n",
    "\n",
    "        # create topic vectors\n",
    "        self._create_topic_vectors(cluster.labels_)\n",
    "\n",
    "        # deduplicate topics\n",
    "        self._deduplicate_topics()\n",
    "\n",
    "        # find topic words and scores\n",
    "        self.topic_words, self.topic_word_scores = self._find_topic_words_and_scores(topic_vectors=self.topic_vectors)\n",
    "\n",
    "        # assign documents to topic\n",
    "        self.doc_top, self.doc_dist = self._calculate_documents_topic(self.topic_vectors,\n",
    "                                                                      self._get_document_vectors())\n",
    "\n",
    "        # calculate topic sizes\n",
    "        self.topic_sizes = self._calculate_topic_sizes(hierarchy=False)\n",
    "\n",
    "        # re-order topics\n",
    "        self._reorder_topics(hierarchy=False)\n",
    "\n",
    "        # initialize variables for hierarchical topic reduction\n",
    "        self.topic_vectors_reduced = None\n",
    "        self.doc_top_reduced = None\n",
    "        self.doc_dist_reduced = None\n",
    "        self.topic_sizes_reduced = None\n",
    "        self.topic_words_reduced = None\n",
    "        self.topic_word_scores_reduced = None\n",
    "        self.hierarchy = None\n",
    "\n",
    "        # initialize document indexing variables\n",
    "        self.document_index = None\n",
    "        self.serialized_document_index = None\n",
    "        self.documents_indexed = False\n",
    "        self.index_id2doc_id = None\n",
    "        self.doc_id2index_id = None\n",
    "\n",
    "        # initialize word indexing variables\n",
    "        self.word_index = None\n",
    "        self.serialized_word_index = None\n",
    "        self.words_indexed = False\n",
    "\n",
    "    def _check_import_status(self):\n",
    "        if self.embedding_model != 'all-mpnet-base-v2':\n",
    "            if not _HAVE_TENSORFLOW:\n",
    "                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n",
    "                                  \"Try: pip install top2vec[sentence_encoders]\\n\\n\"\n",
    "                                  \"Alternatively try: pip install tensorflow tensorflow_hub tensorflow_text\")\n",
    "        else:\n",
    "            if not _HAVE_TORCH:\n",
    "                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n",
    "                                  \"Try: pip install top2vec[sentence_transformers]\\n\\n\"\n",
    "                                  \"Alternatively try: pip install torch sentence_transformers\")\n",
    "\n",
    "    def _check_model_status(self):\n",
    "        if self.embed is None:\n",
    "            if self.verbose is False:\n",
    "                logger.setLevel(logging.DEBUG)\n",
    "\n",
    "            if self.embedding_model != \"all-mpnet-base-v2\":\n",
    "                if self.embedding_model_path is None:\n",
    "                    logger.info(f'Downloading {self.embedding_model} model')\n",
    "                    if self.embedding_model == \"universal-sentence-encoder-multilingual\":\n",
    "                        module = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n",
    "                    else:\n",
    "                        module = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "                else:\n",
    "                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n",
    "                    module = self.embedding_model_path\n",
    "                self.embed = hub.load(module)\n",
    "\n",
    "            else:\n",
    "                if self.embedding_model_path is None:\n",
    "                    logger.info(f'Downloading {self.embedding_model} model')\n",
    "                    module = 'all-mpnet-base-v2'\n",
    "                else:\n",
    "                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n",
    "                    module = self.embedding_model_path\n",
    "                model = SentenceTransformer(module)\n",
    "                self.embed = model.encode\n",
    "\n",
    "        if self.verbose is False:\n",
    "            logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz16gmSjHcs6"
   },
   "source": [
    "We can then use this `Top2VecNew` class to run our experiments including the `\"all-mpnet-base-v2\"` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdPp3Xsda_SJ"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    dataset, custom = \"trump\", True\n",
    "    params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "              # \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "              \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                               'metric': 'euclidean',\n",
    "                               'cluster_selection_method': 'eom'}}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      custom_dataset=custom,\n",
    "                      custom_model=Top2VecNew,\n",
    "                      model_name=\"Top2Vec\",\n",
    "                      params=params,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"Top2Vec_trump_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKxdYyJECpuY"
   },
   "source": [
    "# **DTM Evaluation**\n",
    "\n",
    "Here, we evaluate BERTopic and LDAseq on a dynamic topic modeling task with two datasets: \n",
    "* Trump's tweets\n",
    "* UN general debates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67jrSUR6DPbi"
   },
   "source": [
    "### **BERTopic**\n",
    "\n",
    "As seen before, we can load our data and generate embeddings before passing it to our evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl4xsRDgCtSY"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Prepare data\n",
    "dataset, custom = \"trump_dtm\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snszIP5bIbE4"
   },
   "source": [
    "Then, we also need to make sure that the timestamps match the data that were are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NhqOi6hqJDJ"
   },
   "outputs": [],
   "source": [
    "# Match indices\n",
    "import os\n",
    "os.listdir(f\"./{dataset}\")\n",
    "with open(f\"./{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "timestamps = [timestamp for index, timestamp in enumerate(timestamps) if index in indices]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Luc2euIfVL"
   },
   "source": [
    "Finally, we can simply run the Trainer as we did before but adding the timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqSmtipeIm8n"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    params = {\n",
    "        \"nr_topics\": [50],\n",
    "        \"min_topic_size\": 15,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      bt_timestamps=timestamps,\n",
    "                      topk=5,\n",
    "                      bt_nr_bins=10,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(f\"DynamicBERTopic_trump_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF4AFG2oDQn-"
   },
   "source": [
    "### **LDAseq**\n",
    "To run LDAseq, we again prepare our data and match the indices of our timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ4X6GymFYSu",
    "outputId": "22ab5c57-a279-48de-dc91-966c33a885c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119320it [03:25, 579.62it/s]\n",
      "278837it [00:00, 1751620.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(273743, 273743)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data\n",
    "dataset, custom = \"un_dtm\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Match indices\n",
    "os.listdir(f\"{dataset}\")\n",
    "with open(f\"{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "indices_test = {index: True for index in indices}\n",
    "timestamps = [timestamp for index, timestamp in tqdm(enumerate(timestamps)) if indices_test.get(index)]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAonxNdtJYwD"
   },
   "source": [
    "Then, we simply pass the timestamps and run our the trainer for LDAseq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wv_5-awJ4g7"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"num_topics\": [50],\n",
    "    \"nr_bins\": 9,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"LDAseq\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  bt_timestamps=timestamps,\n",
    "                  topk=5,\n",
    "                  verbose=True)\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNKfK8C3KH1M"
   },
   "source": [
    "We remove some information from the results as those are quite big to save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vB77Xa2qYPZ"
   },
   "outputs": [],
   "source": [
    "results[0][\"Params\"].keys()\n",
    "del results[0][\"Params\"][\"corpus\"]\n",
    "del results[0][\"Params\"][\"id2word\"]\n",
    "del results[0][\"Params\"][\"time_slice\"]\n",
    "\n",
    "import json\n",
    "with open(f\"LDAseq_trump.json\", 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-M13ABjKQYE"
   },
   "source": [
    "# **Wall time**\n",
    "Here, we only focus on the wall time of each topic model, from instantiating the model to training. To do so, we take the Trump dataset and split it up into steps of 1000 documents. Then, we can train a model and track the wall time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7l0Yu9vSKfKc"
   },
   "outputs": [],
   "source": [
    "embedding_model = \"all-mpnet-base-v2\"\n",
    "# embedding_model = tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embedding_model_name = \"all-mpnet-base-v2\"\n",
    "topic_model_name = \"BERTopic_USE\"\n",
    "\n",
    "results = pd.DataFrame(columns=[\"dataset\", \"nr_documents\", \"vocab_size\", \"time\",\n",
    "                                \"cpu\", \"gpu\", \"gpu_cudnn\", \"gpu_memory\", \"embedding_model\"])\n",
    "for index, nr_documents in enumerate(tqdm(np.arange(1000, len(data), 2_000, dtype=int))):\n",
    "    \n",
    "    selected_data = random.sample(data, nr_documents)\n",
    "    selected_tokenized_data = random.sample(tokenized_data, nr_documents)\n",
    "    \n",
    "    if topic_model_name == \"CTM\":\n",
    "        qt, training_dataset = preprocess_ctm(selected_data, embedding_model_name)\n",
    "    \n",
    "    # Run model\n",
    "    start = time.time()\n",
    "    \n",
    "    if topic_model_name == \"LDA\":\n",
    "        id2word = corpora.Dictionary(selected_tokenized_data)\n",
    "        id_corpus = [id2word.doc2bow(document) for document in selected_tokenized_data]\n",
    "        lda = LdaMulticore(id_corpus, id2word=id2word, num_topics=100)\n",
    "    \n",
    "    elif topic_model_name == \"NFM\":\n",
    "        id2word = corpora.Dictionary(selected_tokenized_data)\n",
    "        id_corpus = [id2word.doc2bow(document) for document in selected_tokenized_data]\n",
    "        nmf_model = nmf.Nmf(id_corpus, id2word=id2word, num_topics=100)\n",
    "\n",
    "    elif topic_model_name == \"BERTopic\":\n",
    "        topic_model = BERTopic(embedding_model=embedding_model)    \n",
    "        topics, probs = topic_model.fit_transform(selected_data)\n",
    "        \n",
    "    elif topic_model_name == \"BERTopic_Doc2Vec\":\n",
    "        train_corpus = [TaggedDocument(default_tokenizer(doc), [i]) for i, doc in enumerate(selected_data)]\n",
    "        doc2vec_args = {\"vector_size\": 300,\n",
    "                        \"min_count\": 50,\n",
    "                        \"window\": 15,\n",
    "                        \"sample\": 1e-5,\n",
    "                        \"negative\": 0,\n",
    "                        \"hs\": 1,\n",
    "                        \"epochs\": 40,\n",
    "                        \"dm\": 0,\n",
    "                        \"dbow_words\": 1,\n",
    "                       \"documents\": train_corpus,\n",
    "                       \"workers\": -1}\n",
    "        model = Doc2Vec(**doc2vec_args)\n",
    "        embeddings = model.docvecs.vectors_docs\n",
    "        topic_model = BERTopic()    \n",
    "        topics, probs = topic_model.fit_transform(selected_data, embeddings)\n",
    "        \n",
    "    elif topic_model_name == \"BERTopic_USE\":\n",
    "        embeddings = embedding_model(selected_data).cpu().numpy()\n",
    "        topic_model = BERTopic(embedding_model=embedding_model)    \n",
    "        topics, probs = topic_model.fit_transform(selected_data, embeddings)\n",
    "\n",
    "    elif topic_model_name == \"Top2Vec\":\n",
    "        model = Top2Vec(selected_data, hdbscan_args={\"min_cluster_size\": 15}, workers=-1)\n",
    "#         model = Top2VecNew(selected_data, hdbscan_args={\"min_cluster_size\": 15}, embedding_model=embedding_model)\n",
    "        \n",
    "    elif topic_model_name == \"CTM\":\n",
    "        ctm = CombinedTM(n_components=100, contextual_size=768, bow_size=len(qt.vocab))\n",
    "        ctm.fit(training_dataset)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    # Calculate vocab size\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(selected_data)\n",
    "    vocab_size = len(vectorizer.get_feature_names())\n",
    "    \n",
    "    results.loc[len(results)] = [dataset, len(selected_data), vocab_size, end - start, cpu_name, gpu_name, \n",
    "                                 gpu_cudnn, gpu_memory, embedding_model_name]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERTopic OCTIS v3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "llamaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
