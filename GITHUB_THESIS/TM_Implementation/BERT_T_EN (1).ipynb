{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69b28de-74bf-46d3-a672-8d107b8dc5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arch/RHEL8/EB_production/2022/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "#!pip install bertopic datasets accelerate bitsandbytes xformers adjustText\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Categorical, Integer\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "import time\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "optimizer=Optimizer()\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c231c4-60e1-448b-81d9-17590f92f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "DATA = \"EN\"\n",
    "EM = \"all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6170d343-45b3-4224-84a0-d96c54535ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(DATA)\n",
    "docs = [\" \".join(words) for words in dataset.get_corpus()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5f6dde-d4e8-4539-adfa-86645593571e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6c7ee84d034580b3d438a013f0f9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(EM)\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b4b6481-1473-4668-84c7-1777913e060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, custom = DATA, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7bc9f5-827e-492b-982c-a5fd9ebbdb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 08:10:33.926067: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-07 08:10:34.137285: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pspaargaren/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from evaluation3 import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee882d-610b-4613-98b5-8bd9b2104a9a",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828bc51-d053-4b2c-803d-c9440f01eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TC & TD CALCULATION\n",
    "for i in range(3):\n",
    "    custom = True\n",
    "    params = {\n",
    "        \"embedding_model\": embedding_model,\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 10,\n",
    "        #\"diversity\": None,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                        model_name=\"BERTopic\",\n",
    "                        params=params,\n",
    "                        bt_embeddings=embeddings,\n",
    "                        custom_dataset=custom,\n",
    "                        verbose=True)\n",
    "    results = trainer.train(save=f\"results/EN_mod/scores/bertopic_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba7b6f-d5c9-43b5-bf7b-529f7c083395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPIC EXTRACTION\n",
    "custom = True\n",
    "params = {\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"nr_topics\": 15,\n",
    "    \"min_topic_size\": 10,\n",
    "    #\"diversity\": None,\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                    model_name=\"BERTopic\",\n",
    "                    params=params,\n",
    "                    bt_embeddings=embeddings,\n",
    "                    custom_dataset=custom,\n",
    "                    verbose=True)\n",
    "results = trainer.train(save=f\"results/EN_mod/topics/bertopic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74daf25-3814-4c22-b59f-e975388ca9a6",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332eb7d-dff5-4566-8780-bc5512e54e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = DATA, True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"LDA\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"results/EN_mod/scores/lda_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b77219-4ace-4a54-a5cb-9990a60b0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPIC CREATION:\n",
    "dataset, custom = DATA, True\n",
    "params = {\"num_topics\": 15, \"random_state\": 42}#[(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"LDA\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  verbose=True)\n",
    "results = trainer.train(save=f\"results/EN_mod/topics/lda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a9cb0-32c4-44d3-821e-b2d73c72ae14",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8d3bf-ef38-47c2-8c94-7d5a4ef96bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = DATA, True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"NMF\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"results/EN_mod/scores/nmf_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b00a7-7bb1-454a-850d-9ebf91f4ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPIC CREATION:\n",
    "dataset, custom = DATA, True\n",
    "params = {\"num_topics\": 15, \"random_state\": 42}#[(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"NMF\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  verbose=True)\n",
    "results = trainer.train(save=f\"results/EN_mod/topics/nmf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b978a-3bf1-43b2-974d-0ae430475579",
   "metadata": {},
   "source": [
    "# CTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc43db7-7064-4ad0-b3dc-986b0a74e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a94f3-ab97-49b5-bae6-bbbe51a98bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TC & TD\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "#!pip install contextualized_topic_models\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "for i in range(1):\n",
    "    dataset, custom = DATA, True\n",
    "    params = {\n",
    "        \"n_components\": [(i+1)*10 for i in range(5)],\n",
    "        \"contextual_size\":768\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"CTM_CUSTOM\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"results/EN_mod/scores/ctm_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc889fd-93db-4e26-86fb-f50bcb67dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC CREATION\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "#!pip install contextualized_topic_models\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "#for i in range(3):\n",
    "dataset, custom = DATA, True\n",
    "params = {\n",
    "    \"n_components\": 15,#[(i+1)*10 for i in range(5)],\n",
    "    \"contextual_size\":768\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"CTM_CUSTOM\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  verbose=True)\n",
    "results = trainer.train(save=f\"results/EN_mod/topics/ctm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d4afc-b7f7-4c49-91b9-8eaa60d840a2",
   "metadata": {},
   "source": [
    "# Dynamic TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b0bae35-1a3e-478e-ab02-c249759f7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "dDATA = \"EN_dtm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea297df8-b3b2-47eb-a904-411ad3cb0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(dDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf509225-14a3-4e35-a415-95ee07786581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/pspaargaren/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d664af600f0a45d9ac742e5251343c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prepare data\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from data_t_flower import DataLoader\n",
    "dataset, custom = dDATA, True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "embedding_model = SentenceTransformer(EM) #EM3\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cecd9c23-2762-445d-9cd9-45d3efe533de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match indices\n",
    "import os\n",
    "os.listdir(f\"./{dataset}\")\n",
    "with open(f\"./{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "timestamps = [timestamp for index, timestamp in enumerate(timestamps) if index in indices]\n",
    "#print(timestamps)\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58ed4e31-85e7-4618-ad3e-e63da33cbd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:17:27,918 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-05 12:18:05,343 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-05 12:18:05,345 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-05 12:18:05,783 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-05 12:18:05,783 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-05 12:18:07,040 - BERTopic - Representation - Completed ✓\n",
      "2024-06-05 12:18:07,041 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-05 12:18:08,116 - BERTopic - Topic reduction - Reduced number of topics from 230 to 10\n",
      "5it [00:02,  2.25it/s]\n",
      "2024-06-05 12:19:13,546 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-05 12:19:25,496 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-05 12:19:25,498 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-05 12:19:25,937 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-05 12:19:25,938 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-05 12:19:27,207 - BERTopic - Representation - Completed ✓\n",
      "2024-06-05 12:19:27,207 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-05 12:19:28,237 - BERTopic - Topic reduction - Reduced number of topics from 255 to 20\n",
      "5it [00:02,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrttemberg\n",
      "tiar\n",
      "nymjih\n",
      "lry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:20:34,726 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-05 12:20:47,463 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-05 12:20:47,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-05 12:20:47,899 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-05 12:20:47,899 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-05 12:20:49,176 - BERTopic - Representation - Completed ✓\n",
      "2024-06-05 12:20:49,177 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-05 12:20:50,218 - BERTopic - Topic reduction - Reduced number of topics from 270 to 30\n",
      "5it [00:02,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blachandrika\n",
      "lry\n",
      "rasnjana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:21:51,413 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-05 12:22:10,897 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-05 12:22:10,899 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-05 12:22:11,345 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-05 12:22:11,346 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-05 12:22:12,634 - BERTopic - Representation - Completed ✓\n",
      "2024-06-05 12:22:12,635 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-05 12:22:13,695 - BERTopic - Topic reduction - Reduced number of topics from 259 to 40\n",
      "5it [00:01,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abb\n",
      "lry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 12:23:13,305 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-05 12:23:25,402 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-05 12:23:25,403 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-05 12:23:25,850 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-05 12:23:25,850 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-05 12:23:27,143 - BERTopic - Representation - Completed ✓\n",
      "2024-06-05 12:23:27,144 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-05 12:23:28,228 - BERTopic - Topic reduction - Reduced number of topics from 248 to 50\n",
      "5it [00:02,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abb\n",
      "blachandrika\n",
      "thas\n",
      "lry\n"
     ]
    }
   ],
   "source": [
    "from evaluation import Trainer\n",
    "params = {\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 5,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      bt_timestamps=timestamps,\n",
    "                      topk=5,\n",
    "                      bt_nr_bins=5,\n",
    "                      verbose=True)\n",
    "results = trainer.train(f\"D_EN_T_EVO_M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f9fd0c-afa5-47a8-a0b3-3a934d326dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
