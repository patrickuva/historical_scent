{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e69b28de-74bf-46d3-a672-8d107b8dc5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bertopic datasets accelerate bitsandbytes xformers adjustText\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Categorical, Integer\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "import time\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "optimizer=Optimizer()\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee56130c-5436-4715-9530-c9ae2f1e0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load your pre-trained model\n",
    "model_name = 'emanjavacas/GysBERT'\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Define the custom pooling layer with mean and max pooling combined\n",
    "class CustomPoolingLayer(models.Pooling):\n",
    "    def __init__(self, word_embedding_dimension):\n",
    "        super(CustomPoolingLayer, self).__init__(word_embedding_dimension)\n",
    "    \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def max_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "        max_embeddings = torch.max(token_embeddings, 1)[0]\n",
    "        return max_embeddings\n",
    "\n",
    "    def forward(self, features):\n",
    "        token_embeddings = features['token_embeddings']\n",
    "        attention_mask = features['attention_mask']\n",
    "        \n",
    "        # Mean pooling\n",
    "        mean_pooled = self.mean_pooling(token_embeddings, attention_mask)\n",
    "        \n",
    "        # Max pooling\n",
    "        max_pooled = self.max_pooling(token_embeddings, attention_mask)\n",
    "        \n",
    "        # Concatenate mean and max pooled embeddings\n",
    "        sentence_embedding = torch.cat((mean_pooled, max_pooled), 1)\n",
    "        \n",
    "        features.update({'sentence_embedding': sentence_embedding})\n",
    "        return features\n",
    "\n",
    "# Get the embedding dimension\n",
    "embedding_dim = word_embedding_model.get_word_embedding_dimension()\n",
    "\n",
    "# Create the custom pooling layer\n",
    "custom_pooling = CustomPoolingLayer(embedding_dim)\n",
    "\n",
    "# Define the SentenceTransformer model with the custom pooling\n",
    "sentence_transformer_model = SentenceTransformer(modules=[word_embedding_model, custom_pooling])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96c231c4-60e1-448b-81d9-17590f92f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "DATA = \"NL\"\n",
    "dDATA = \"NL_dtm\"\n",
    "EM = \"emanjavacas/GysBERT\"\n",
    "EM2 = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "#EM3 = sentence_transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6170d343-45b3-4224-84a0-d96c54535ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(DATA)\n",
    "docs = [\" \".join(words) for words in dataset.get_corpus()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba5f6dde-d4e8-4539-adfa-86645593571e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbaccd1d76445f28f21d9bbd3d4f198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/596 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(EM2) #EM3\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b4b6481-1473-4668-84c7-1777913e060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, custom = DATA, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f7bc9f5-827e-492b-982c-a5fd9ebbdb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation3 import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce80ec04-5d09-45a4-be5c-a33370a2dc11",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47aa8ebe-24d7-47c0-8aa0-ef185d409f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 14:53:58,530 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-17 14:54:09,054 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-17 14:54:09,055 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-17 14:54:09,188 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-17 14:54:09,189 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-17 14:54:11,929 - BERTopic - Representation - Completed ✓\n",
      "2024-06-17 14:54:11,931 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-17 14:54:11,932 - BERTopic - Topic reduction - Reduced number of topics from 8 to 8\n",
      "2024-06-17 14:54:23,466 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.03958452182863852\n",
      "diversity: 0.6428571428571429\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 14:54:38,908 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-17 14:54:38,909 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-17 14:54:39,058 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-17 14:54:39,059 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-17 14:54:41,813 - BERTopic - Representation - Completed ✓\n",
      "2024-06-17 14:54:41,815 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-17 14:54:41,816 - BERTopic - Topic reduction - Reduced number of topics from 8 to 8\n",
      "2024-06-17 14:54:53,413 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.020784525555494347\n",
      "diversity: 0.6714285714285714\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 14:55:07,249 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-17 14:55:07,250 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-17 14:55:12,590 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-17 14:55:12,590 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-17 14:55:15,413 - BERTopic - Representation - Completed ✓\n",
      "2024-06-17 14:55:15,415 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-17 14:55:15,416 - BERTopic - Topic reduction - Reduced number of topics from 8 to 8\n",
      "2024-06-17 14:55:27,405 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.039438096127898316\n",
      "diversity: 0.7285714285714285\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 14:55:41,215 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-17 14:55:41,217 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-17 14:55:44,871 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-17 14:55:44,872 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-17 14:55:47,668 - BERTopic - Representation - Completed ✓\n",
      "2024-06-17 14:55:47,670 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-17 14:55:47,671 - BERTopic - Topic reduction - Reduced number of topics from 8 to 8\n",
      "2024-06-17 14:55:59,604 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.02691926249087866\n",
      "diversity: 0.6857142857142857\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 14:56:12,463 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-17 14:56:12,465 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-17 14:56:17,726 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-17 14:56:17,727 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-17 14:56:20,505 - BERTopic - Representation - Completed ✓\n",
      "2024-06-17 14:56:20,507 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-17 14:56:20,508 - BERTopic - Topic reduction - Reduced number of topics from 8 to 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.024355885605067136\n",
      "diversity: 0.7\n",
      " \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/kmeans/scores/bertopicNL_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 19\u001b[0m\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     13\u001b[0m                     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m                     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m     15\u001b[0m                     bt_embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m     16\u001b[0m                     custom_dataset\u001b[38;5;241m=\u001b[39mcustom,\n\u001b[1;32m     17\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#results = trainer.train(save=f\"results/NL/scores/bertopic_{i+1}\")\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/kmeans/scores/bertopicNL_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/home3/pspaargaren/Pats_BERT_LLAMA/Tryout/evaluation_kmeans.py:128\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, save)\u001b[0m\n\u001b[1;32m    125\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    129\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(results, f)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/kmeans/scores/bertopicNL_1.json'"
     ]
    }
   ],
   "source": [
    "#TC & TD CALCULATION\n",
    "for i in range(1):\n",
    "    custom = True\n",
    "    params = {\n",
    "        \"embedding_model\": embedding_model,\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 10,\n",
    "        #\"diversity\": None,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                        model_name=\"BERTopic\",\n",
    "                        params=params,\n",
    "                        bt_embeddings=embeddings,\n",
    "                        custom_dataset=custom,\n",
    "                        verbose=True)\n",
    "    results = trainer.train(save=f\"results/NL/scores/bertopic_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2b306-7f0d-4235-9678-43926bcde354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPIC EXTRACTION\n",
    "custom = True\n",
    "params = {\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"nr_topics\": 15,\n",
    "    \"min_topic_size\": 10,\n",
    "    #\"diversity\": None,\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                    model_name=\"BERTopic\",\n",
    "                    params=params,\n",
    "                    bt_embeddings=embeddings,\n",
    "                    custom_dataset=custom,\n",
    "                    verbose=True)\n",
    "results = trainer.train(save=f\"results/NL/topics/bertopic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f135c-261c-40b6-913d-e55bb9babd96",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f21f3-2eb9-4652-9af3-005bf6b196ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = DATA, True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"LDA\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"results/NL/scores/lda_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90424e89-e9a7-43c1-a01f-c7eb9c1d3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPIC CREATION:\n",
    "dataset, custom = DATA, True\n",
    "params = {\"num_topics\": 15, \"random_state\": 42}#[(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"LDA\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  verbose=True)\n",
    "results = trainer.train(save=f\"results/NL/topics/lda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b22f7-479e-4b3d-ae1b-f77ad237a15a",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2cdcb-c16f-45d8-8ada-cadab087b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = DATA, True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"NMF\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"results/NL/scores/nmf_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680c868-f7fa-4012-83ae-41aec6590376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPIC CREATION:\n",
    "dataset, custom = DATA, True\n",
    "params = {\"num_topics\": 15, \"random_state\": 42}#[(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"NMF\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  verbose=True)\n",
    "results = trainer.train(save=f\"results/NL/topics/nmf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88812ca7-fc3c-405b-8a73-afb73f1236aa",
   "metadata": {},
   "source": [
    "# CTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac4667-d55a-4a9c-908a-9db3ffdefe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9077e1da-3857-44d7-affd-831464f98f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TC & TD\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "#!pip install contextualized_topic_models\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "for i in range(1):\n",
    "    dataset, custom = DATA, True\n",
    "    params = {\n",
    "        \"n_components\": [(i+1)*10 for i in range(5)],\n",
    "        \"contextual_size\":768\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"CTM_CUSTOM\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"results/NL/scores/ctm_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acee31-94d6-47db-8aa2-41ff1a1269c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC CREATION\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "#!pip install contextualized_topic_models\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "#for i in range(3):\n",
    "dataset, custom = DATA, True\n",
    "params = {\n",
    "    \"n_components\": 15,#[(i+1)*10 for i in range(5)],\n",
    "    \"contextual_size\":768\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"CTM_CUSTOM\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  verbose=True)\n",
    "results = trainer.train(save=f\"results/NL/topics/ctm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51286bd1-eb13-4159-af2c-6f0d930f991b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dynamic TM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ee792-5e19-4423-a746-132dc1990bec",
   "metadata": {},
   "source": [
    "## HISTORICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d2a7c-d429-47d9-b3d8-e1d074530100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "dDATA = \"NL_dtm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb31f5-31f3-4205-aa0f-e19d499281a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(dDATA)\n",
    "data = [\" \".join(words) for words in dataset.get_corpus()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4d01c-47ed-47af-aee0-3210c03c0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, custom = dDATA, True\n",
    "from data_NL import DataLoader\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "embedding_model = SentenceTransformer(EM) #EM3\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836426c-e279-4961-b05b-93c7dc702222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match indices\n",
    "import os\n",
    "os.listdir(f\"./{dataset}\")\n",
    "with open(f\"./{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "timestamps = [timestamp for index, timestamp in enumerate(timestamps) if index in indices]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f702a7-53d9-4c20-8b7d-d31c30351993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import Trainer\n",
    "#for i in range(3):\n",
    "params = {\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 5,\n",
    "        \"verbose\": True #,\n",
    "        #\"evolution_tuning\": True,\n",
    "        #\"global_tuning\": False\n",
    "    }\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      bt_timestamps=timestamps,\n",
    "                      topk=5,\n",
    "                      bt_nr_bins=5,\n",
    "                      verbose=True)\n",
    "results = trainer.train(f\"D_NL_all-H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f3da7-635f-4335-a831-ac6388fc5d2e",
   "metadata": {},
   "source": [
    "# MODERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df99e1-3f1a-4939-94d1-08d9083e6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "dDATA = \"NL_dtm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fccd02d-e964-49cd-a673-a50086221575",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(dDATA)\n",
    "data = [\" \".join(words) for words in dataset.get_corpus()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda40f61-cc78-428f-b649-2536d9db19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, custom = dDATA, True\n",
    "from data_NL import DataLoader\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "embedding_model = SentenceTransformer(EM2) #EM3\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561cac1-c1cb-4342-8a17-dfa24512e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match indices\n",
    "import os\n",
    "os.listdir(f\"./{dataset}\")\n",
    "with open(f\"./{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "timestamps = [timestamp for index, timestamp in enumerate(timestamps) if index in indices]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b3a11-cd38-4965-be65-076863e30acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import Trainer\n",
    "#for i in range(3):\n",
    "params = {\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 5,\n",
    "        \"verbose\": True #,\n",
    "        # \"evolution_tuning\": True,\n",
    "        # \"global_tuning\": False\n",
    "    }\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      bt_timestamps=timestamps,\n",
    "                      topk=5,\n",
    "                      bt_nr_bins=5,\n",
    "                      verbose=True)\n",
    "results = trainer.train(f\"D_NL_all_M\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
