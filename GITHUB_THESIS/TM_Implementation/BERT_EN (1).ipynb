{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e69b28de-74bf-46d3-a672-8d107b8dc5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arch/RHEL8/EB_production/2022/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "#!pip install bertopic datasets accelerate bitsandbytes xformers adjustText\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Categorical, Integer\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "import time\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "optimizer=Optimizer()\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b22c031-3506-4a47-9f7a-fb372d0ea984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load your pre-trained model\n",
    "model_name = 'emanjavacas/MacBERTh'\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Define the custom pooling layer with mean and max pooling combined\n",
    "class CustomPoolingLayer(models.Pooling):\n",
    "    def __init__(self, word_embedding_dimension):\n",
    "        super(CustomPoolingLayer, self).__init__(word_embedding_dimension)\n",
    "    \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def max_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "        max_embeddings = torch.max(token_embeddings, 1)[0]\n",
    "        return max_embeddings\n",
    "\n",
    "    def forward(self, features):\n",
    "        token_embeddings = features['token_embeddings']\n",
    "        attention_mask = features['attention_mask']\n",
    "        \n",
    "        # Mean pooling\n",
    "        mean_pooled = self.mean_pooling(token_embeddings, attention_mask)\n",
    "        \n",
    "        # Max pooling\n",
    "        max_pooled = self.max_pooling(token_embeddings, attention_mask)\n",
    "        \n",
    "        # Concatenate mean and max pooled embeddings\n",
    "        sentence_embedding = torch.cat((mean_pooled, max_pooled), 1)\n",
    "        \n",
    "        features.update({'sentence_embedding': sentence_embedding})\n",
    "        return features\n",
    "\n",
    "# Get the embedding dimension\n",
    "embedding_dim = word_embedding_model.get_word_embedding_dimension()\n",
    "\n",
    "# Create the custom pooling layer\n",
    "custom_pooling = CustomPoolingLayer(embedding_dim)\n",
    "\n",
    "# Define the SentenceTransformer model with the custom pooling\n",
    "sentence_transformer_model = SentenceTransformer(modules=[word_embedding_model, custom_pooling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c231c4-60e1-448b-81d9-17590f92f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "DATA = \"EN\"\n",
    "EM = \"emanjavacas/MacBERTh\"\n",
    "EM2 = \"all-mpnet-base-v2\"\n",
    "#EM3 = sentence_transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6170d343-45b3-4224-84a0-d96c54535ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(DATA)\n",
    "docs = [\" \".join(words) for words in dataset.get_corpus()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba5f6dde-d4e8-4539-adfa-86645593571e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becd6879c3544c7482e0b582ca88b654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(EM2) #EM3\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b4b6481-1473-4668-84c7-1777913e060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, custom = DATA, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f7bc9f5-827e-492b-982c-a5fd9ebbdb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 15:19:42.422360: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-17 15:19:42.808512: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pspaargaren/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from evaluation_kmeans import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4fc00-5cfb-4b94-94ba-72d9124bbca8",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4314fb23-a438-4503-b23d-9530036c26a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 15:19:52,445 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-17 15:20:24,071 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-17 15:20:24,072 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-17 15:20:24,263 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-17 15:20:24,265 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-17 15:20:25,249 - BERTopic - Representation - Completed ✓\n",
      "2024-06-17 15:20:25,250 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-17 15:20:25,251 - BERTopic - Topic reduction - Reduced number of topics from 8 to 8\n",
      "2024-06-17 15:20:32,079 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.05982443598337828\n",
      "diversity: 0.6142857142857143\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 15:20:50,189 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-17 15:20:50,190 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-17 15:20:50,282 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-17 15:20:50,283 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-17 15:20:51,294 - BERTopic - Representation - Completed ✓\n",
      "2024-06-17 15:20:51,294 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-17 15:20:51,296 - BERTopic - Topic reduction - Reduced number of topics from 8 to 8\n",
      "2024-06-17 15:20:58,172 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.054036331518551105\n",
      "diversity: 0.6714285714285714\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 15:21:10,011 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-17 15:21:10,012 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-17 15:21:10,114 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-17 15:21:10,116 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-17 15:21:11,107 - BERTopic - Representation - Completed ✓\n",
      "2024-06-17 15:21:11,108 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-17 15:21:11,109 - BERTopic - Topic reduction - Reduced number of topics from 8 to 8\n",
      "2024-06-17 15:21:18,015 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.05556631870419089\n",
      "diversity: 0.6428571428571429\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 15:21:30,899 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-17 15:21:30,900 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-17 15:21:30,996 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-17 15:21:30,998 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-17 15:21:31,993 - BERTopic - Representation - Completed ✓\n",
      "2024-06-17 15:21:31,994 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-17 15:21:31,995 - BERTopic - Topic reduction - Reduced number of topics from 8 to 8\n",
      "2024-06-17 15:21:38,840 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.038774305763555464\n",
      "diversity: 0.6428571428571429\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 15:21:51,580 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-17 15:21:51,582 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-17 15:21:51,680 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-17 15:21:51,682 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-17 15:21:52,668 - BERTopic - Representation - Completed ✓\n",
      "2024-06-17 15:21:52,669 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-17 15:21:52,670 - BERTopic - Topic reduction - Reduced number of topics from 8 to 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.05770491383436944\n",
      "diversity: 0.6714285714285714\n",
      " \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/kmeans/scores/bertopicNL_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 19\u001b[0m\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     13\u001b[0m                     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m                     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m     15\u001b[0m                     bt_embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m     16\u001b[0m                     custom_dataset\u001b[38;5;241m=\u001b[39mcustom,\n\u001b[1;32m     17\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#results = trainer.train(save=f\"results/EN/scores/bertopic_{i+1}\")\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/kmeans/scores/bertopicNL_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/home3/pspaargaren/Pats_BERT_LLAMA/Tryout/evaluation_kmeans.py:128\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, save)\u001b[0m\n\u001b[1;32m    125\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    129\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(results, f)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/kmeans/scores/bertopicNL_1.json'"
     ]
    }
   ],
   "source": [
    "#TC & TD CALCULATION\n",
    "for i in range(1):\n",
    "    custom = True\n",
    "    params = {\n",
    "        \"embedding_model\": embedding_model,\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 10,\n",
    "        #\"diversity\": None,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                        model_name=\"BERTopic\",\n",
    "                        params=params,\n",
    "                        bt_embeddings=embeddings,\n",
    "                        custom_dataset=custom,\n",
    "                        verbose=True)\n",
    "    #results = trainer.train(save=f\"results/EN/scores/bertopic_{i+1}\")\n",
    "    results = trainer.train(save=f\"results/kmeans/scores/bertopicNL_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3bb703-07db-473b-8202-b39030209f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPIC EXTRACTION\n",
    "custom = True\n",
    "params = {\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"nr_topics\": 15,\n",
    "    \"min_topic_size\": 10,\n",
    "    #\"diversity\": None,\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                    model_name=\"BERTopic\",\n",
    "                    params=params,\n",
    "                    bt_embeddings=embeddings,\n",
    "                    custom_dataset=custom,\n",
    "                    verbose=True)\n",
    "results = trainer.train(save=f\"results/EN/topics/bertopic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a85cc71-f1b3-484d-bbb6-ebe8da428abd",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256d0cb-8be6-4c3a-b70e-b53ccd1c3a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = DATA, True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"LDA\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"results/EN/scores/lda_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3de834-fdfb-4e90-b04b-91a96bb79ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPIC CREATION:\n",
    "dataset, custom = DATA, True\n",
    "params = {\"num_topics\": 15, \"random_state\": 42}#[(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"LDA\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  verbose=True)\n",
    "results = trainer.train(save=f\"results/EN/topics/lda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dbc43b-9123-481c-b60a-62cd5c3f8b8d",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e66d20-8b2e-4a75-8f2e-9fb3e13ced83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = DATA, True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"NMF\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"results/EN/scores/nmf_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df727cd-b18f-462e-be3e-9768199f9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPIC CREATION:\n",
    "dataset, custom = DATA, True\n",
    "params = {\"num_topics\": 15, \"random_state\": 42}#[(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"NMF\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  verbose=True)\n",
    "results = trainer.train(save=f\"results/EN/topics/nmf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68adb8-6117-4c7c-97fb-34933a3b52fd",
   "metadata": {},
   "source": [
    "# CTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa49e9-71a1-4bc6-a666-35daf9a26cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6bf35-da8a-40f3-b12d-4cbd0db3a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TC & TD\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "#!pip install contextualized_topic_models\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "for i in range(1):\n",
    "    dataset, custom = DATA, True\n",
    "    params = {\n",
    "        \"n_components\": [(i+1)*10 for i in range(5)],\n",
    "        \"contextual_size\":768\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"CTM_CUSTOM\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"results/EN/scores/ctm_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193eee22-eb55-4117-b87d-6919f8517007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC CREATION\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "#!pip install contextualized_topic_models\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "#for i in range(3):\n",
    "dataset, custom = DATA, True\n",
    "params = {\n",
    "    \"n_components\": 15,#[(i+1)*10 for i in range(5)],\n",
    "    \"contextual_size\":768\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"CTM_CUSTOM\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  verbose=True)\n",
    "results = trainer.train(save=f\"results/EN/topics/ctm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f959e1-0361-4a4b-9033-f58b21b36ef7",
   "metadata": {},
   "source": [
    "# Dynamic TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ac6049a-8ba2-4400-b91e-cb7e9b4545da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "dDATA = \"EN_dtm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331e5920-07b3-4081-a7b9-53ea2ace3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(dDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37d39964-d45b-42a6-92c9-61a2be2627f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/pspaargaren/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "No sentence-transformers model found with name emanjavacas/MacBERTh. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6316ee7e974df0a73fc1e35894e4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data\n",
    "from data_flower import DataLoader\n",
    "dataset, custom = dDATA, True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "embedding_model = SentenceTransformer(EM) #EM3\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6356667-9c89-4687-8c16-cabf7dd5c309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46236, 46236)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match indices\n",
    "import os\n",
    "os.listdir(f\"./{dataset}\")\n",
    "with open(f\"./{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "timestamps = [timestamp for index, timestamp in enumerate(timestamps) if index in indices]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19c5c1ba-5bfd-4afa-8e6c-620535ddfd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 16:04:46,721 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-03 16:05:31,135 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-03 16:05:31,136 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-03 16:05:36,081 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-03 16:05:36,082 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-03 16:05:42,744 - BERTopic - Representation - Completed ✓\n",
      "2024-06-03 16:05:42,746 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-03 16:05:48,044 - BERTopic - Topic reduction - Reduced number of topics from 1276 to 10\n",
      "5it [00:05,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marjo\n",
      "bcautify\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 16:11:00,431 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-03 16:11:24,226 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-03 16:11:24,228 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-03 16:11:28,787 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-03 16:11:28,788 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-03 16:11:35,389 - BERTopic - Representation - Completed ✓\n",
      "2024-06-03 16:11:35,393 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-03 16:11:40,798 - BERTopic - Topic reduction - Reduced number of topics from 1281 to 20\n",
      "5it [00:05,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regula\n",
      "baiga\n",
      "stepe\n",
      "bcautify\n",
      "orlanois\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 16:16:25,809 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-03 16:17:01,500 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-03 16:17:01,503 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-03 16:17:06,483 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-03 16:17:06,484 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-03 16:17:13,102 - BERTopic - Representation - Completed ✓\n",
      "2024-06-03 16:17:13,105 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-03 16:17:18,426 - BERTopic - Topic reduction - Reduced number of topics from 1282 to 30\n",
      "5it [00:05,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regula\n",
      "marjo\n",
      "baiga\n",
      "stepe\n",
      "bcautify\n",
      "orlanois\n",
      "apcides\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 16:21:42,645 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-03 16:22:06,393 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-03 16:22:06,395 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-03 16:22:11,808 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-03 16:22:11,809 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-03 16:22:18,568 - BERTopic - Representation - Completed ✓\n",
      "2024-06-03 16:22:18,572 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-03 16:22:24,172 - BERTopic - Topic reduction - Reduced number of topics from 1291 to 40\n",
      "5it [00:07,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regula\n",
      "marjo\n",
      "baiga\n",
      "stepe\n",
      "semelinesse\n",
      "swere\n",
      "sicle\n",
      "apcides\n",
      "bcautify\n",
      "orlanois\n",
      "prd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 16:27:27,129 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-03 16:27:52,652 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-03 16:27:52,654 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-03 16:27:57,014 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-03 16:27:57,015 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-03 16:28:03,701 - BERTopic - Representation - Completed ✓\n",
      "2024-06-03 16:28:03,704 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-03 16:28:09,154 - BERTopic - Topic reduction - Reduced number of topics from 1274 to 50\n",
      "5it [00:06,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regula\n",
      "marjo\n",
      "sepul\n",
      "baiga\n",
      "stepe\n",
      "mazus\n",
      "ernor\n",
      "bcautify\n",
      "orlanois\n",
      "apcides\n",
      "prd\n"
     ]
    }
   ],
   "source": [
    "from evaluation import Trainer\n",
    "\n",
    "params = {\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 5,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      bt_timestamps=timestamps,\n",
    "                      topk=5,\n",
    "                      bt_nr_bins=5,\n",
    "                      verbose=True)\n",
    "results = trainer.train(f\"DynamicBERTopic_EN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d3fbf-0cd4-486f-8ecf-fd6d13871cbc",
   "metadata": {},
   "source": [
    "# MODERN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e1753-b678-4cff-8f8e-ddb9fb3c7c58",
   "metadata": {},
   "source": [
    "# MODERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3775266-893e-47ef-859b-1258be30f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "dDATA = \"EN_dtm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd953b46-8182-4fe7-8b89-04da8cfd2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(dDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3cf0ca5-82aa-4147-91c4-a3cf9580ce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/pspaargaren/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082ff4422c0b404cb11266de9c407a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data\n",
    "from data_flower import DataLoader\n",
    "dataset, custom = dDATA, True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "embedding_model = SentenceTransformer(EM2) #EM3\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a1eef54-1a1c-4713-844d-97eb9ac3136a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46236, 46236)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match indices\n",
    "import os\n",
    "os.listdir(f\"./{dataset}\")\n",
    "with open(f\"./{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "timestamps = [timestamp for index, timestamp in enumerate(timestamps) if index in indices]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26a03373-3e95-4d98-8dcb-c73c54478438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:18:57,095 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-04 08:19:46,755 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-04 08:19:46,758 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-04 08:19:52,043 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-04 08:19:52,044 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-04 08:19:59,389 - BERTopic - Representation - Completed ✓\n",
      "2024-06-04 08:19:59,392 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-04 08:20:05,298 - BERTopic - Topic reduction - Reduced number of topics from 1308 to 10\n",
      "5it [00:04,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marjo\n",
      "baiga\n",
      "embellifli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:24:06,663 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-04 08:24:33,236 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-04 08:24:33,239 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-04 08:24:36,462 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-04 08:24:36,463 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-04 08:24:43,775 - BERTopic - Representation - Completed ✓\n",
      "2024-06-04 08:24:43,778 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-04 08:24:49,735 - BERTopic - Topic reduction - Reduced number of topics from 1345 to 20\n",
      "5it [00:05,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regula\n",
      "baiga\n",
      "longitudi\n",
      "embellifli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:28:54,694 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-04 08:29:20,922 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-04 08:29:20,925 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-04 08:29:24,190 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-04 08:29:24,191 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-04 08:29:31,548 - BERTopic - Representation - Completed ✓\n",
      "2024-06-04 08:29:31,551 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-04 08:29:37,694 - BERTopic - Topic reduction - Reduced number of topics from 1349 to 30\n",
      "5it [00:06,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regula\n",
      "marjo\n",
      "baiga\n",
      "stepe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:34:22,503 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-04 08:34:58,959 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-04 08:34:58,963 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-04 08:35:03,487 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-04 08:35:03,488 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-04 08:35:10,919 - BERTopic - Representation - Completed ✓\n",
      "2024-06-04 08:35:10,923 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-04 08:35:17,036 - BERTopic - Topic reduction - Reduced number of topics from 1364 to 40\n",
      "5it [00:07,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regula\n",
      "baiga\n",
      "stepe\n",
      "marjo\n",
      "thof\n",
      "encyclopdia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:40:18,315 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-04 08:40:46,224 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-04 08:40:46,227 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-04 08:40:50,684 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-04 08:40:50,685 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-04 08:40:58,176 - BERTopic - Representation - Completed ✓\n",
      "2024-06-04 08:40:58,181 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-04 08:41:04,504 - BERTopic - Topic reduction - Reduced number of topics from 1340 to 50\n",
      "5it [00:08,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regula\n",
      "marjo\n",
      "sepul\n",
      "baiga\n",
      "stepe\n"
     ]
    }
   ],
   "source": [
    "from evaluation import Trainer\n",
    "#for i in range(3):\n",
    "params = {\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 5,\n",
    "        \"verbose\": True #,\n",
    "        # \"evolution_tuning\": True,\n",
    "        # \"global_tuning\": False\n",
    "    }\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      bt_timestamps=timestamps,\n",
    "                      topk=5,\n",
    "                      bt_nr_bins=5,\n",
    "                      verbose=True)\n",
    "results = trainer.train(f\"D_EN_NO_M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82b17ee9-ac6e-428b-9eec-df1fca30f9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Het is klaar\n"
     ]
    }
   ],
   "source": [
    "print(\"Het is klaar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18bd20-9af3-4b4b-94a8-1a98102f1a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
