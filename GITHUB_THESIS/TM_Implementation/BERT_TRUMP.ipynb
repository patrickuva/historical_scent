{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69b28de-74bf-46d3-a672-8d107b8dc5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arch/RHEL8/EB_production/2022/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "#!pip install bertopic datasets accelerate bitsandbytes xformers adjustText\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Categorical, Integer\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "import time\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "optimizer=Optimizer()\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b22c031-3506-4a47-9f7a-fb372d0ea984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load your pre-trained model\n",
    "model_name = 'vinai/bertweet-base'\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Define the custom pooling layer with mean and max pooling combined\n",
    "class CustomPoolingLayer(models.Pooling):\n",
    "    def __init__(self, word_embedding_dimension):\n",
    "        super(CustomPoolingLayer, self).__init__(word_embedding_dimension)\n",
    "    \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def max_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "        max_embeddings = torch.max(token_embeddings, 1)[0]\n",
    "        return max_embeddings\n",
    "\n",
    "    def forward(self, features):\n",
    "        token_embeddings = features['token_embeddings']\n",
    "        attention_mask = features['attention_mask']\n",
    "        \n",
    "        # Mean pooling\n",
    "        mean_pooled = self.mean_pooling(token_embeddings, attention_mask)\n",
    "        \n",
    "        # Max pooling\n",
    "        max_pooled = self.max_pooling(token_embeddings, attention_mask)\n",
    "        \n",
    "        # Concatenate mean and max pooled embeddings\n",
    "        sentence_embedding = torch.cat((mean_pooled, max_pooled), 1)\n",
    "        \n",
    "        features.update({'sentence_embedding': sentence_embedding})\n",
    "        return features\n",
    "\n",
    "# Get the embedding dimension\n",
    "embedding_dim = word_embedding_model.get_word_embedding_dimension()\n",
    "\n",
    "# Create the custom pooling layer\n",
    "custom_pooling = CustomPoolingLayer(embedding_dim)\n",
    "\n",
    "# Define the SentenceTransformer model with the custom pooling\n",
    "sentence_transformer_model = SentenceTransformer(modules=[word_embedding_model, custom_pooling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96c231c4-60e1-448b-81d9-17590f92f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "DATA = \"trump\"\n",
    "EM = \"vinai/bertweet-base\"\n",
    "EM2 = \"all-mpnet-base-v1\"\n",
    "EM3 = sentence_transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38caaa4b-c940-401f-a645-c1bfcbe50ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/pspaargaren/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from data_trump import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6170d343-45b3-4224-84a0-d96c54535ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7d43f58aec42e9aa79bd10e80e8c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc3641a0a884925b5d17c3b22a8aca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c170725da8e4522862fcb140dbb5826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b102c411f2f84dfaa3b1b436109bf2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140852e6a1fa40948a9febbf22ecd39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/591 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939b02f376d7410fb8f081efba0d01e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f94175394e4de8a4e3a1c48f954e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435c423de3ba45f4a2816b2b14405e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722db30056ef44b9a1648236c6a8075d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a006e6dc29454bef8c2a60e5343cdd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48332079cf52429e87e262f186d0584d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f12eae10f3f4fd1bc5e9bf03bebdc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# Prepare data\n",
    "dataset, custom = \"trump\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba5f6dde-d4e8-4539-adfa-86645593571e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42adb4650a8a40eab1ce2c24a74fb61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(DATA)\n",
    "docs = [\" \".join(words) for words in dataset.get_corpus()]\n",
    "embedding_model = EM3#SentenceTransformer(EM2) #EM3\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b4b6481-1473-4668-84c7-1777913e060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, custom = DATA, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7bc9f5-827e-492b-982c-a5fd9ebbdb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:39:32.971971: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-11 19:39:33.406317: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pspaargaren/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from evaluation_trump import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545067f2-b11c-4e97-bd76-a3c3ee11ff8b",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d058485-fcf2-4a6d-aba3-4b7d2e13ad9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:40:19,326 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:40:43,956 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:40:43,959 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-11 19:40:47,253 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:40:47,254 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:40:48,426 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:40:48,427 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:40:49,308 - BERTopic - Topic reduction - Reduced number of topics from 347 to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.001786529033318741\n",
      "diversity: 0.6777777777777778\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:40:57,033 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:41:20,142 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:41:20,145 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-11 19:41:22,149 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:41:22,150 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:41:23,319 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:41:23,320 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:41:24,195 - BERTopic - Topic reduction - Reduced number of topics from 350 to 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.02716461578672935\n",
      "diversity: 0.7105263157894737\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:41:32,556 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:41:57,249 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:41:57,251 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-11 19:41:59,279 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:41:59,280 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:42:00,466 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:42:00,467 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:42:01,364 - BERTopic - Topic reduction - Reduced number of topics from 347 to 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.018471848338267063\n",
      "diversity: 0.696551724137931\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:42:10,103 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:42:43,820 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:42:43,823 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-11 19:42:45,971 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:42:45,972 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:42:47,138 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:42:47,139 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:42:48,033 - BERTopic - Topic reduction - Reduced number of topics from 340 to 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.056991451337893205\n",
      "diversity: 0.735897435897436\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:42:57,213 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:43:30,450 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:43:30,453 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-11 19:43:32,407 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:43:32,408 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:43:33,562 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:43:33,563 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:43:34,467 - BERTopic - Topic reduction - Reduced number of topics from 340 to 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.028162978374618394\n",
      "diversity: 0.710204081632653\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:43:45,194 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:44:08,580 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:44:08,583 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-11 19:44:10,507 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:44:10,508 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:44:11,661 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:44:11,662 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:44:12,520 - BERTopic - Topic reduction - Reduced number of topics from 342 to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.041638212005307096\n",
      "diversity: 0.7\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:44:20,264 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:44:44,285 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:44:44,288 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-11 19:44:46,219 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:44:46,219 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:44:47,373 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:44:47,374 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:44:48,250 - BERTopic - Topic reduction - Reduced number of topics from 344 to 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.03409288314938726\n",
      "diversity: 0.7578947368421053\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:44:56,852 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:45:30,968 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:45:30,970 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-11 19:45:32,891 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:45:32,891 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:45:34,060 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:45:34,060 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:45:34,965 - BERTopic - Topic reduction - Reduced number of topics from 349 to 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.050106209004822835\n",
      "diversity: 0.7586206896551724\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:45:43,823 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:46:07,225 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:46:07,228 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-11 19:46:09,283 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:46:09,283 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:46:10,439 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:46:10,439 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:46:11,329 - BERTopic - Topic reduction - Reduced number of topics from 340 to 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.03847502592173673\n",
      "diversity: 0.7769230769230769\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:46:20,882 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:46:44,488 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:46:44,490 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-11 19:46:46,511 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:46:46,512 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:46:47,660 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:46:47,660 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:46:48,563 - BERTopic - Topic reduction - Reduced number of topics from 333 to 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.03924466838829158\n",
      "diversity: 0.7346938775510204\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:46:59,163 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:47:22,855 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:47:22,857 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-11 19:47:24,865 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:47:24,866 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:47:26,013 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:47:26,013 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:47:26,882 - BERTopic - Topic reduction - Reduced number of topics from 339 to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.005349066786059384\n",
      "diversity: 0.6444444444444445\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:47:34,513 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:47:58,805 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:47:58,808 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-11 19:48:00,858 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:48:00,859 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:48:01,991 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:48:01,992 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:48:02,858 - BERTopic - Topic reduction - Reduced number of topics from 347 to 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.04894062064103474\n",
      "diversity: 0.7473684210526316\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:48:11,334 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 19:48:34,469 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 19:48:34,471 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-11 19:48:36,462 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 19:48:36,462 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 19:48:37,620 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 19:48:37,621 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 19:48:38,513 - BERTopic - Topic reduction - Reduced number of topics from 349 to 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.021781731775790816\n",
      "diversity: 0.7517241379310344\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:48:47,382 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "CPUDispatcher(<function nn_descent at 0x15466ee28ca0>) returned a result with an exception set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numba/core/serialize.py:30\u001b[0m, in \u001b[0;36m_numba_unpickle\u001b[0;34m(address, bytedata, hashed)\u001b[0m\n\u001b[1;32m     27\u001b[0m _unpickled_memo \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numba_unpickle\u001b[39m(address, bytedata, hashed):\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Used by `numba_unpickle` from _helperlib.c\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m        unpickled object\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mSystemError\u001b[0m: _PyEval_EvalFrameDefault returned a result with an exception set",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 23\u001b[0m\n\u001b[1;32m      8\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-mpnet-base-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnr_topics\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     17\u001b[0m                     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m                     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m                     representation_model \u001b[38;5;241m=\u001b[39m representation_model,\n\u001b[1;32m     22\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 23\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/trump/scores/TR_bertopic_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/home3/pspaargaren/Pats_BERT_LLAMA/Tryout/evaluation_trump.py:166\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, save)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_combo \u001b[38;5;129;01min\u001b[39;00m new_params:\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# Train and evaluate model\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     params_to_use \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    164\u001b[0m         param: value \u001b[38;5;28;01mfor\u001b[39;00m param, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_name, param_combo)\n\u001b[1;32m    165\u001b[0m     }\n\u001b[0;32m--> 166\u001b[0m     output, computation_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_tm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_to_use\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(output)\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# Update results\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home3/pspaargaren/Pats_BERT_LLAMA/Tryout/evaluation_trump.py:205\u001b[0m, in \u001b[0;36mTrainer._train_tm_model\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Train BERTopic\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_bertopic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Train Top2Vec\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop2Vec\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/gpfs/home3/pspaargaren/Pats_BERT_LLAMA/Tryout/evaluation_trump.py:411\u001b[0m, in \u001b[0;36mTrainer._train_bertopic\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    408\u001b[0m     model \u001b[38;5;241m=\u001b[39m BERTopic(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    410\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 411\u001b[0m topics, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Dynamic Topic Modeling\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamps:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bertopic/_bertopic.py:408\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_zeroshot_topics(documents, assigned_documents, assigned_embeddings)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Reduce dimensionality\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m umap_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce_dimensionality\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Cluster reduced embeddings\u001b[39;00m\n\u001b[1;32m    411\u001b[0m documents, probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_embeddings(umap_embeddings, documents, y\u001b[38;5;241m=\u001b[39my)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bertopic/_bertopic.py:3355\u001b[0m, in \u001b[0;36mBERTopic._reduce_dimensionality\u001b[0;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[1;32m   3352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3353\u001b[0m     \u001b[38;5;66;03m# cuml umap needs y to be an numpy array\u001b[39;00m\n\u001b[1;32m   3354\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y) \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3355\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mumap_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mumap_model\u001b[38;5;241m.\u001b[39mfit(embeddings)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/umap/umap_.py:2612\u001b[0m, in \u001b[0;36mUMAP.fit\u001b[0;34m(self, X, y, force_all_finite)\u001b[0m\n\u001b[1;32m   2606\u001b[0m     nn_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_distance_func\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknn_dists \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2608\u001b[0m     (\n\u001b[1;32m   2609\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_knn_indices,\n\u001b[1;32m   2610\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_knn_dists,\n\u001b[1;32m   2611\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_knn_search_index,\n\u001b[0;32m-> 2612\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mnearest_neighbors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2614\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnn_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2616\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2617\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mangular_rp_forest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2620\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_pynndescent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_knn_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknn_indices\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/umap/umap_.py:330\u001b[0m, in \u001b[0;36mnearest_neighbors\u001b[0;34m(X, n_neighbors, metric, metric_kwds, angular, random_state, low_memory, use_pynndescent, n_jobs, verbose)\u001b[0m\n\u001b[1;32m    327\u001b[0m     n_trees \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m20.0\u001b[39m)))\n\u001b[1;32m    328\u001b[0m     n_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mlog2(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))))\n\u001b[0;32m--> 330\u001b[0m     knn_search_index \u001b[38;5;241m=\u001b[39m \u001b[43mNNDescent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompressed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m     knn_indices, knn_dists \u001b[38;5;241m=\u001b[39m knn_search_index\u001b[38;5;241m.\u001b[39mneighbor_graph\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pynndescent/pynndescent_.py:946\u001b[0m, in \u001b[0;36mNNDescent.__init__\u001b[0;34m(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, max_rptree_depth, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose)\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    944\u001b[0m         \u001b[38;5;28mprint\u001b[39m(ts(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNN descent for\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(n_iters), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 946\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_neighbor_graph \u001b[38;5;241m=\u001b[39m \u001b[43mnn_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43meffective_max_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrp_tree_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_init_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleaf_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleaf_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_neighbor_graph[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    962\u001b[0m     warn(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to correctly find n_neighbors for some samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Results may be less than ideal. Try re-running with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m different parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    966\u001b[0m     )\n",
      "\u001b[0;31mSystemError\u001b[0m: CPUDispatcher(<function nn_descent at 0x15466ee28ca0>) returned a result with an exception set"
     ]
    }
   ],
   "source": [
    "#TC & TD CALCULATION\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "\n",
    "representation_model = MaximalMarginalRelevance(diversity=None)\n",
    "for i in range(3):\n",
    "    custom = True\n",
    "    params = {\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 15,\n",
    "        #\"diversity\": None,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                        model_name=\"BERTopic\",\n",
    "                        params=params,\n",
    "                        bt_embeddings=embeddings,\n",
    "                        custom_dataset=custom,\n",
    "                        representation_model = representation_model,\n",
    "                        verbose=True)\n",
    "    results = trainer.train(save=f\"results/trump/scores/TR_bertopic_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87201a85-4eae-43cd-bed2-8afbc662099f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 11:19:42,805 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-11 11:20:06,796 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-11 11:20:06,799 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-11 11:20:10,857 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-11 11:20:10,858 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-11 11:20:12,207 - BERTopic - Representation - Completed ✓\n",
      "2024-06-11 11:20:12,208 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-11 11:20:13,088 - BERTopic - Topic reduction - Reduced number of topics from 549 to 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.04377524861085428\n",
      "diversity: 0.8071428571428572\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#TOPIC EXTRACTION\n",
    "custom = True\n",
    "params = {\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"nr_topics\": 15,\n",
    "    \"min_topic_size\": 10,\n",
    "    #\"diversity\": None,\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                    model_name=\"BERTopic\",\n",
    "                    params=params,\n",
    "                    bt_embeddings=embeddings,\n",
    "                    custom_dataset=custom,\n",
    "                    verbose=True)\n",
    "results = trainer.train(save=f\"results/trump/topics/TR_bertopic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db3a4b-b367-48a6-84ad-765c66c9080b",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "293338ae-ba8c-49ff-8bb0-8db574a4ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.00691144547486858\n",
      "diversity: 0.48\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.005003465267478509\n",
      "diversity: 0.425\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.004333717218804874\n",
      "diversity: 0.5133333333333333\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.013484627735113932\n",
      "diversity: 0.5475\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.02873932987802867\n",
      "diversity: 0.556\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.008732834718608936\n",
      "diversity: 0.41\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.0032195570281142545\n",
      "diversity: 0.44\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.011184204528511061\n",
      "diversity: 0.52\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.010834766671676585\n",
      "diversity: 0.5275\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.020161391882537752\n",
      "diversity: 0.596\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.004250980433560531\n",
      "diversity: 0.45\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.006889694250977099\n",
      "diversity: 0.47\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.005643503573800874\n",
      "diversity: 0.48333333333333334\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.01100933809236911\n",
      "diversity: 0.55\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.018123237177004415\n",
      "diversity: 0.574\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = DATA, True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"LDA\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"results/trump/scores/TR_lda_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18f47b38-37e4-480c-ba5d-fd25faa13dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.005812970405126746\n",
      "diversity: 0.3933333333333333\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#TOPIC CREATION:\n",
    "dataset, custom = DATA, True\n",
    "params = {\"num_topics\": 15, \"random_state\": 42}#[(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"LDA\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  verbose=True)\n",
    "results = trainer.train(save=f\"results/trump/topics/TR_lda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f91831-434f-40ea-9a27-7d49b4109c81",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b0303f-fcea-49f8-95e0-c9956fbf62e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.005742839116935675\n",
      "diversity: 0.4\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.007421120689733075\n",
      "diversity: 0.42\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.014766098200964957\n",
      "diversity: 0.3933333333333333\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.016208854270394913\n",
      "diversity: 0.3375\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.01722279340526775\n",
      "diversity: 0.348\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.00391705970481047\n",
      "diversity: 0.46\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.007975363716605935\n",
      "diversity: 0.415\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.02100240700031534\n",
      "diversity: 0.38\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.010374395636827634\n",
      "diversity: 0.36\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.00693951977530063\n",
      "diversity: 0.316\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.00437487653220107\n",
      "diversity: 0.38\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.0064257546686909885\n",
      "diversity: 0.405\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.011036886533741021\n",
      "diversity: 0.37666666666666665\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.01675117935273563\n",
      "diversity: 0.3575\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.015734180346554376\n",
      "diversity: 0.342\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = DATA, True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"NMF\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"results/trump/scores/TR_nmf_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26219be9-cd3b-4244-9188-9aaa7bfed6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.004451174752642762\n",
      "diversity: 0.38\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#TOPIC CREATION:\n",
    "dataset, custom = DATA, True\n",
    "params = {\"num_topics\": 15, \"random_state\": 42}#[(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"NMF\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  verbose=True)\n",
    "results = trainer.train(save=f\"results/trump/topics/TR_nmf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59309d17-f09e-4c65-b94b-b3167f0ed4d6",
   "metadata": {},
   "source": [
    "# CTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb3bb8b-b5a5-45e5-b071-b5112432a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb1343-84cc-423c-8938-29120c598f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f55de4a5824b49bfbf777dcf847211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [4422400/4425200]\tTrain Loss: 47.67496832200311\tTime: 0:00:09.658039: : 100it [16:20,  9.80s/it]\n",
      "100%|██████████| 692/692 [00:08<00:00, 84.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.00824862720803795\n",
      "diversity: 0.99\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [4422400/4425200]\tTrain Loss: 50.25803543068738\tTime: 0:00:09.565081: : 100it [16:22,  9.83s/it]\n",
      "100%|██████████| 692/692 [00:07<00:00, 87.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.006199995908221468\n",
      "diversity: 0.96\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [68/100]\t Seen Samples: [3007232/4425200]\tTrain Loss: 53.692074385117174\tTime: 0:00:09.632899: : 68it [11:13,  9.65s/it]"
     ]
    }
   ],
   "source": [
    "# TC & TD\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "#!pip install contextualized_topic_models\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "for i in range(1):\n",
    "    dataset, custom = DATA, True\n",
    "    params = {\n",
    "        \"n_components\": [(i+1)*10 for i in range(5)],\n",
    "        \"contextual_size\":768\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"CTM_CUSTOM\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"results/trump/scores/ctm_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad770f7-4cf3-42e3-9041-3f980608c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC CREATION\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "#!pip install contextualized_topic_models\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "#for i in range(3):\n",
    "dataset, custom = DATA, True\n",
    "params = {\n",
    "    \"n_components\": 15,#[(i+1)*10 for i in range(5)],\n",
    "    \"contextual_size\":768\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"CTM_CUSTOM\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  verbose=True)\n",
    "results = trainer.train(save=f\"results/trump/topics/TR_ctm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f959e1-0361-4a4b-9033-f58b21b36ef7",
   "metadata": {},
   "source": [
    "# Dynamic TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ac6049a-8ba2-4400-b91e-cb7e9b4545da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "dDATA = \"trump_dtm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "331e5920-07b3-4081-a7b9-53ea2ace3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(dDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37d39964-d45b-42a6-92c9-61a2be2627f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/pspaargaren/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "No sentence-transformers model found with name emanjavacas/MacBERTh. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1642c675e139470287ad33039c5e49ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data\n",
    "from data_trump import DataLoader\n",
    "dataset, custom = dDATA, True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "embedding_model = SentenceTransformer(EM2) #EM3\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6356667-9c89-4687-8c16-cabf7dd5c309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44252, 44252)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match indices\n",
    "import os\n",
    "os.listdir(f\"./{dataset}\")\n",
    "with open(f\"./{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "timestamps = [timestamp for index, timestamp in enumerate(timestamps) if index in indices]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19c5c1ba-5bfd-4afa-8e6c-620535ddfd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pspaargaren/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2024-06-10 23:01:33,786 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-10 23:01:58,810 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-10 23:01:58,812 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-10 23:02:02,127 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-10 23:02:02,128 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-10 23:02:03,773 - BERTopic - Representation - Completed ✓\n",
      "2024-06-10 23:02:03,774 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-10 23:02:04,650 - BERTopic - Topic reduction - Reduced number of topics from 947 to 10\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1241094607596999936",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 18\u001b[0m\n\u001b[1;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnr_topics\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;66;03m#[(i+1)*10 for i in range(5)],\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_topic_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     }\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     10\u001b[0m                       model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m                       params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m                       bt_nr_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     17\u001b[0m                       verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDynamicBERTopic_EN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/home3/pspaargaren/Pats_BERT_LLAMA/Tryout/evaluation.py:165\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, save)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_combo \u001b[38;5;129;01min\u001b[39;00m new_params:\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# Train and evaluate model\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     params_to_use \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    163\u001b[0m         param: value \u001b[38;5;28;01mfor\u001b[39;00m param, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_name, param_combo)\n\u001b[1;32m    164\u001b[0m     }\n\u001b[0;32m--> 165\u001b[0m     output, computation_time, topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_tm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_to_use\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(output)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Update results\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home3/pspaargaren/Pats_BERT_LLAMA/Tryout/evaluation.py:206\u001b[0m, in \u001b[0;36mTrainer._train_tm_model\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Train BERTopic\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 206\u001b[0m     output, computation_time, topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_bertopic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output, computation_time, topics\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Train Top2Vec\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home3/pspaargaren/Pats_BERT_LLAMA/Tryout/evaluation.py:436\u001b[0m, in \u001b[0;36mTrainer._train_bertopic\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Dynamic Topic Modeling\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamps:\n\u001b[0;32m--> 436\u001b[0m     topics_over_time \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopics_over_time\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnr_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnr_bins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevolution_tuning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_tuning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     unique_timestamps \u001b[38;5;241m=\u001b[39m topics_over_time\u001b[38;5;241m.\u001b[39mTimestamp\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m    445\u001b[0m     dtm_topics \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bertopic/_bertopic.py:828\u001b[0m, in \u001b[0;36mBERTopic.topics_over_time\u001b[0;34m(self, docs, timestamps, topics, nr_bins, datetime_format, evolution_tuning, global_tuning)\u001b[0m\n\u001b[1;32m    824\u001b[0m topic_frequency \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(documents_per_topic\u001b[38;5;241m.\u001b[39mTimestamps\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m    825\u001b[0m                             index\u001b[38;5;241m=\u001b[39mdocuments_per_topic\u001b[38;5;241m.\u001b[39mTopic)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;66;03m# Fill dataframe with results\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m topics_at_timestamp \u001b[38;5;241m=\u001b[39m [(topic,\n\u001b[1;32m    829\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([words[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m words \u001b[38;5;129;01min\u001b[39;00m values][:\u001b[38;5;241m5\u001b[39m]),\n\u001b[1;32m    830\u001b[0m                         topic_frequency[topic],\n\u001b[1;32m    831\u001b[0m                         timestamp) \u001b[38;5;28;01mfor\u001b[39;00m topic, values \u001b[38;5;129;01min\u001b[39;00m words_per_topic\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    832\u001b[0m topics_over_time\u001b[38;5;241m.\u001b[39mextend(topics_at_timestamp)\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evolution_tuning:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bertopic/_bertopic.py:830\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    824\u001b[0m topic_frequency \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(documents_per_topic\u001b[38;5;241m.\u001b[39mTimestamps\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m    825\u001b[0m                             index\u001b[38;5;241m=\u001b[39mdocuments_per_topic\u001b[38;5;241m.\u001b[39mTopic)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;66;03m# Fill dataframe with results\u001b[39;00m\n\u001b[1;32m    828\u001b[0m topics_at_timestamp \u001b[38;5;241m=\u001b[39m [(topic,\n\u001b[1;32m    829\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([words[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m words \u001b[38;5;129;01min\u001b[39;00m values][:\u001b[38;5;241m5\u001b[39m]),\n\u001b[0;32m--> 830\u001b[0m                         \u001b[43mtopic_frequency\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    831\u001b[0m                         timestamp) \u001b[38;5;28;01mfor\u001b[39;00m topic, values \u001b[38;5;129;01min\u001b[39;00m words_per_topic\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    832\u001b[0m topics_over_time\u001b[38;5;241m.\u001b[39mextend(topics_at_timestamp)\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evolution_tuning:\n",
      "\u001b[0;31mKeyError\u001b[0m: 1241094607596999936"
     ]
    }
   ],
   "source": [
    "from evaluation import Trainer\n",
    "\n",
    "params = {\n",
    "        \"nr_topics\": 10, #[(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 5,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      bt_timestamps=timestamps,\n",
    "                      topk=5,\n",
    "                      bt_nr_bins=5,\n",
    "                      verbose=True)\n",
    "results = trainer.train(f\"DynamicBERTopic_EN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d3fbf-0cd4-486f-8ecf-fd6d13871cbc",
   "metadata": {},
   "source": [
    "# MODERN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e1753-b678-4cff-8f8e-ddb9fb3c7c58",
   "metadata": {},
   "source": [
    "# MODERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3775266-893e-47ef-859b-1258be30f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "dDATA = \"EN_dtm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd953b46-8182-4fe7-8b89-04da8cfd2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(dDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3cf0ca5-82aa-4147-91c4-a3cf9580ce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/pspaargaren/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082ff4422c0b404cb11266de9c407a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data\n",
    "from data_flower import DataLoader\n",
    "dataset, custom = dDATA, True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "embedding_model = SentenceTransformer(EM2) #EM3\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a1eef54-1a1c-4713-844d-97eb9ac3136a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46236, 46236)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match indices\n",
    "import os\n",
    "os.listdir(f\"./{dataset}\")\n",
    "with open(f\"./{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "timestamps = [timestamp for index, timestamp in enumerate(timestamps) if index in indices]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26a03373-3e95-4d98-8dcb-c73c54478438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:18:57,095 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-04 08:19:46,755 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-04 08:19:46,758 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-04 08:19:52,043 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-04 08:19:52,044 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-04 08:19:59,389 - BERTopic - Representation - Completed ✓\n",
      "2024-06-04 08:19:59,392 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-04 08:20:05,298 - BERTopic - Topic reduction - Reduced number of topics from 1308 to 10\n",
      "5it [00:04,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marjo\n",
      "baiga\n",
      "embellifli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:24:06,663 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-04 08:24:33,236 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-04 08:24:33,239 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-04 08:24:36,462 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-04 08:24:36,463 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-04 08:24:43,775 - BERTopic - Representation - Completed ✓\n",
      "2024-06-04 08:24:43,778 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-04 08:24:49,735 - BERTopic - Topic reduction - Reduced number of topics from 1345 to 20\n",
      "5it [00:05,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regula\n",
      "baiga\n",
      "longitudi\n",
      "embellifli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:28:54,694 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-04 08:29:20,922 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-04 08:29:20,925 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-06-04 08:29:24,190 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-04 08:29:24,191 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-04 08:29:31,548 - BERTopic - Representation - Completed ✓\n",
      "2024-06-04 08:29:31,551 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-04 08:29:37,694 - BERTopic - Topic reduction - Reduced number of topics from 1349 to 30\n",
      "5it [00:06,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regula\n",
      "marjo\n",
      "baiga\n",
      "stepe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:34:22,503 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-04 08:34:58,959 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-04 08:34:58,963 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-04 08:35:03,487 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-04 08:35:03,488 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-04 08:35:10,919 - BERTopic - Representation - Completed ✓\n",
      "2024-06-04 08:35:10,923 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-04 08:35:17,036 - BERTopic - Topic reduction - Reduced number of topics from 1364 to 40\n",
      "5it [00:07,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regula\n",
      "baiga\n",
      "stepe\n",
      "marjo\n",
      "thof\n",
      "encyclopdia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:40:18,315 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-04 08:40:46,224 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-04 08:40:46,227 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-04 08:40:50,684 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-04 08:40:50,685 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-06-04 08:40:58,176 - BERTopic - Representation - Completed ✓\n",
      "2024-06-04 08:40:58,181 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-06-04 08:41:04,504 - BERTopic - Topic reduction - Reduced number of topics from 1340 to 50\n",
      "5it [00:08,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regula\n",
      "marjo\n",
      "sepul\n",
      "baiga\n",
      "stepe\n"
     ]
    }
   ],
   "source": [
    "from evaluation import Trainer\n",
    "#for i in range(3):\n",
    "params = {\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 5,\n",
    "        \"verbose\": True #,\n",
    "        # \"evolution_tuning\": True,\n",
    "        # \"global_tuning\": False\n",
    "    }\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      bt_timestamps=timestamps,\n",
    "                      topk=5,\n",
    "                      bt_nr_bins=5,\n",
    "                      verbose=True)\n",
    "results = trainer.train(f\"D_EN_NO_M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82b17ee9-ac6e-428b-9eec-df1fca30f9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Het is klaar\n"
     ]
    }
   ],
   "source": [
    "print(\"Het is klaar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18bd20-9af3-4b4b-94a8-1a98102f1a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
