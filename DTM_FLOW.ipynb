{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a57d67f-d4a4-4995-8c70-9a09801336d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "#!pip install bertopic datasets accelerate bitsandbytes xformers adjustText\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd736f99-5925-4d0b-a320-39895d7b74a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arch/RHEL8/EB_production/2022/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from octis.models.CTM import CTM\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Categorical, Integer\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.dataset.dataset import Dataset\n",
    "from skopt.space.space import Real, Categorical, Integer\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "optimizer=Optimizer()\n",
    "import time\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "dataset = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805bcb0c-7409-4593-a6f0-4b841c7292d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/pspaargaren/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "369628ed-f15d-46b7-8b9f-03d5b1df5ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46236/46236 [00:00<00:00, 115295.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocab\n",
      "127126\n",
      "words filtering done\n"
     ]
    }
   ],
   "source": [
    "#Normal Loader\n",
    "dataloader = DataLoader(dataset=\"EN\").prepare_docs(save=\"EN.txt\").preprocess_octis(output_folder=\"EN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861629f3-8085-494e-959b-afb00681399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46236/46236 [00:00<00:00, 114438.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocab\n",
      "127126\n",
      "words filtering done\n"
     ]
    }
   ],
   "source": [
    "#DTM Loader\n",
    "dataloader = DataLoader(dataset=\"EN_dtm\").prepare_docs(save=\"EN_dtm.txt\").preprocess_octis(output_folder=\"EN_dtm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fc29440-b53f-4e20-a884-ad45de98f8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c77da6d9cbb42c1bdacac7d0826d439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Prepare data\n",
    "dataset, custom = \"EN_dtm\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6be2eaef-c75b-44e0-aa7d-1c04a3f25729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46236, 46236)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match indices\n",
    "import os\n",
    "os.listdir(f\"./{dataset}\")\n",
    "with open(f\"./{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "timestamps = [timestamp for index, timestamp in enumerate(timestamps) if index in indices]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e7a047c-6301-446f-bd21-95e277553c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 12:01:20.650051: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-14 12:01:21.113304: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-14 12:01:35,108 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-05-14 12:02:20,197 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-05-14 12:02:20,199 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-05-14 12:02:25,735 - BERTopic - Cluster - Completed ✓\n",
      "2024-05-14 12:02:25,736 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-05-14 12:02:32,268 - BERTopic - Representation - Completed ✓\n",
      "2024-05-14 12:02:32,271 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-05-14 12:02:37,546 - BERTopic - Topic reduction - Reduced number of topics from 1330 to 10\n",
      "5it [00:03,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marjo\n"
     ]
    }
   ],
   "source": [
    "from evaluation import Trainer\n",
    "params = {\n",
    "        \"nr_topics\": 10,\n",
    "        \"min_topic_size\": 5,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      bt_timestamps=timestamps,\n",
    "                      topk=5,\n",
    "                      bt_nr_bins=5,\n",
    "                      verbose=True)\n",
    "results = trainer.train(f\"DynamicBERTopic_EN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff45ef33-ed63-4b74-85bb-babfb74b6e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 17:06:02.465915: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-10 17:06:02.875432: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from evaluation import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09649e53-19c6-4896-aa20-993154f649cf",
   "metadata": {},
   "source": [
    "## LDAseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf2819cb-ecb9-4893-b404-cecd0d085e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19058, 19058)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data\n",
    "dataset, custom = \"NL_dtm\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Match indices\n",
    "os.listdir(f\"{dataset}\")\n",
    "with open(f\"{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "indices_test = {index: True for index in indices}\n",
    "timestamps = [timestamp for index, timestamp in enumerate(timestamps) if indices_test.get(index)]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28922ed2-1721-4ec8-a4d5-9bebfe49c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA_seq speedup\n",
    "#!pip uninstall gensim -y\n",
    "#!pip install git+https://github.com/RaRe-Technologies/gensim.git@refs/pull/3172/merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aac99f-e61b-4be6-9644-1c11c43b5725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19058 19058 [ 521 1060 1104  934 1278  969 1224 1560 3521 6887]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pspaargaren/.local/lib/python3.10/site-packages/gensim/models/ldaseqmodel.py:288: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  convergence = np.fabs((bound - old_bound) / old_bound)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"num_topics\": 10,\n",
    "    \"nr_bins\": 10,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"LDAseq\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  bt_timestamps=timestamps,\n",
    "                  topk=5,\n",
    "                  verbose=True)\n",
    "results = trainer.train(f\"LDASeq_NL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6968c2-8f7f-440f-9eef-bf0a7964e4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06077aa0-ce20-4704-bf44-38f3e3bb2f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
